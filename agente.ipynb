{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4) (120,) (30, 4) (30,)\n",
      "[[0.02777778 0.375      0.06779661 0.04166667]\n",
      " [0.16666667 0.20833333 0.59322034 0.66666667]\n",
      " [0.69444444 0.33333333 0.6440678  0.54166667]\n",
      " [0.16666667 0.45833333 0.08474576 0.        ]\n",
      " [0.33333333 0.20833333 0.50847458 0.5       ]\n",
      " [0.55555556 0.20833333 0.6779661  0.75      ]\n",
      " [0.36111111 0.29166667 0.54237288 0.5       ]\n",
      " [0.55555556 0.33333333 0.69491525 0.58333333]\n",
      " [0.94444444 0.41666667 0.86440678 0.91666667]\n",
      " [0.94444444 0.75       0.96610169 0.875     ]\n",
      " [0.91666667 0.41666667 0.94915254 0.83333333]\n",
      " [0.47222222 0.375      0.59322034 0.58333333]\n",
      " [0.19444444 0.         0.42372881 0.375     ]\n",
      " [0.41666667 0.29166667 0.52542373 0.375     ]\n",
      " [0.41666667 0.25       0.50847458 0.45833333]\n",
      " [0.11111111 0.5        0.10169492 0.04166667]\n",
      " [0.22222222 0.75       0.08474576 0.08333333]\n",
      " [0.66666667 0.20833333 0.81355932 0.70833333]\n",
      " [0.58333333 0.45833333 0.76271186 0.70833333]\n",
      " [0.30555556 0.58333333 0.08474576 0.125     ]\n",
      " [0.55555556 0.20833333 0.66101695 0.58333333]\n",
      " [0.22222222 0.75       0.10169492 0.04166667]\n",
      " [1.         0.75       0.91525424 0.79166667]\n",
      " [0.19444444 0.41666667 0.10169492 0.04166667]\n",
      " [0.33333333 0.16666667 0.45762712 0.375     ]\n",
      " [0.55555556 0.375      0.77966102 0.70833333]\n",
      " [0.58333333 0.29166667 0.72881356 0.75      ]\n",
      " [0.19444444 0.58333333 0.10169492 0.125     ]\n",
      " [0.36111111 0.33333333 0.66101695 0.79166667]\n",
      " [0.19444444 0.625      0.05084746 0.08333333]\n",
      " [0.22222222 0.70833333 0.08474576 0.125     ]\n",
      " [0.5        0.33333333 0.50847458 0.5       ]\n",
      " [0.5        0.41666667 0.61016949 0.54166667]\n",
      " [0.38888889 1.         0.08474576 0.125     ]\n",
      " [0.72222222 0.45833333 0.74576271 0.83333333]\n",
      " [0.52777778 0.58333333 0.74576271 0.91666667]\n",
      " [0.38888889 0.25       0.42372881 0.375     ]\n",
      " [0.25       0.29166667 0.49152542 0.54166667]\n",
      " [0.66666667 0.54166667 0.79661017 1.        ]\n",
      " [0.44444444 0.5        0.6440678  0.70833333]\n",
      " [0.08333333 0.58333333 0.06779661 0.08333333]\n",
      " [0.75       0.5        0.62711864 0.54166667]\n",
      " [0.30555556 0.79166667 0.05084746 0.125     ]\n",
      " [0.86111111 0.33333333 0.86440678 0.75      ]\n",
      " [0.16666667 0.41666667 0.06779661 0.04166667]\n",
      " [0.22222222 0.625      0.06779661 0.08333333]\n",
      " [0.41666667 0.29166667 0.69491525 0.75      ]\n",
      " [0.19444444 0.5        0.03389831 0.04166667]\n",
      " [0.05555556 0.125      0.05084746 0.08333333]\n",
      " [0.13888889 0.41666667 0.06779661 0.        ]\n",
      " [0.13888889 0.41666667 0.06779661 0.08333333]\n",
      " [0.36111111 0.375      0.44067797 0.5       ]\n",
      " [0.55555556 0.54166667 0.84745763 1.        ]\n",
      " [0.38888889 0.33333333 0.59322034 0.5       ]\n",
      " [0.22222222 0.625      0.06779661 0.04166667]\n",
      " [0.44444444 0.41666667 0.69491525 0.70833333]\n",
      " [0.38888889 0.33333333 0.52542373 0.5       ]\n",
      " [0.55555556 0.58333333 0.77966102 0.95833333]\n",
      " [0.08333333 0.5        0.06779661 0.04166667]\n",
      " [0.41666667 0.29166667 0.69491525 0.75      ]\n",
      " [0.33333333 0.625      0.05084746 0.04166667]\n",
      " [0.33333333 0.125      0.50847458 0.5       ]\n",
      " [0.38888889 0.20833333 0.6779661  0.79166667]\n",
      " [0.13888889 0.58333333 0.15254237 0.04166667]\n",
      " [0.47222222 0.29166667 0.69491525 0.625     ]\n",
      " [0.33333333 0.25       0.57627119 0.45833333]\n",
      " [0.94444444 0.33333333 0.96610169 0.79166667]\n",
      " [0.36111111 0.41666667 0.59322034 0.58333333]\n",
      " [0.61111111 0.33333333 0.61016949 0.58333333]\n",
      " [0.58333333 0.5        0.72881356 0.91666667]\n",
      " [0.22222222 0.58333333 0.08474576 0.04166667]\n",
      " [0.22222222 0.54166667 0.11864407 0.16666667]\n",
      " [0.19444444 0.66666667 0.06779661 0.04166667]\n",
      " [0.47222222 0.08333333 0.6779661  0.58333333]\n",
      " [0.41666667 0.29166667 0.49152542 0.45833333]\n",
      " [0.80555556 0.41666667 0.81355932 0.625     ]\n",
      " [0.33333333 0.16666667 0.47457627 0.41666667]\n",
      " [0.66666667 0.41666667 0.71186441 0.91666667]\n",
      " [0.55555556 0.29166667 0.66101695 0.70833333]\n",
      " [0.47222222 0.58333333 0.59322034 0.625     ]\n",
      " [0.16666667 0.45833333 0.08474576 0.04166667]\n",
      " [0.41666667 0.33333333 0.69491525 0.95833333]\n",
      " [0.52777778 0.08333333 0.59322034 0.58333333]\n",
      " [0.19444444 0.625      0.10169492 0.20833333]\n",
      " [0.72222222 0.5        0.79661017 0.91666667]\n",
      " [0.25       0.875      0.08474576 0.        ]\n",
      " [0.80555556 0.66666667 0.86440678 1.        ]\n",
      " [0.22222222 0.20833333 0.33898305 0.41666667]\n",
      " [0.66666667 0.45833333 0.62711864 0.58333333]\n",
      " [0.13888889 0.45833333 0.10169492 0.04166667]\n",
      " [0.52777778 0.375      0.55932203 0.5       ]\n",
      " [0.61111111 0.5        0.69491525 0.79166667]\n",
      " [0.22222222 0.75       0.15254237 0.125     ]\n",
      " [0.         0.41666667 0.01694915 0.        ]\n",
      " [0.66666667 0.54166667 0.79661017 0.83333333]\n",
      " [0.94444444 0.25       1.         0.91666667]\n",
      " [0.69444444 0.41666667 0.76271186 0.83333333]\n",
      " [0.55555556 0.125      0.57627119 0.5       ]\n",
      " [0.52777778 0.33333333 0.6440678  0.70833333]\n",
      " [0.30555556 0.79166667 0.11864407 0.125     ]\n",
      " [0.69444444 0.5        0.83050847 0.91666667]\n",
      " [0.38888889 0.375      0.54237288 0.5       ]\n",
      " [0.80555556 0.5        0.84745763 0.70833333]\n",
      " [0.77777778 0.41666667 0.83050847 0.83333333]\n",
      " [0.27777778 0.70833333 0.08474576 0.04166667]\n",
      " [0.58333333 0.375      0.55932203 0.5       ]\n",
      " [0.5        0.33333333 0.62711864 0.45833333]\n",
      " [0.38888889 0.41666667 0.54237288 0.45833333]\n",
      " [0.36111111 0.41666667 0.52542373 0.5       ]\n",
      " [0.66666667 0.45833333 0.57627119 0.54166667]\n",
      " [0.25       0.625      0.08474576 0.04166667]\n",
      " [0.58333333 0.33333333 0.77966102 0.83333333]\n",
      " [0.47222222 0.08333333 0.50847458 0.375     ]\n",
      " [0.44444444 0.41666667 0.54237288 0.58333333]\n",
      " [0.13888889 0.58333333 0.10169492 0.04166667]\n",
      " [0.16666667 0.66666667 0.06779661 0.        ]\n",
      " [0.11111111 0.5        0.05084746 0.04166667]\n",
      " [0.33333333 0.91666667 0.06779661 0.04166667]\n",
      " [0.72222222 0.45833333 0.66101695 0.58333333]\n",
      " [0.08333333 0.45833333 0.08474576 0.04166667]]\n"
     ]
    }
   ],
   "source": [
    "from qiskit import QuantumCircuit\n",
    "from qiskit.visualization import plot_histogram\n",
    "from qiskit_machine_learning.algorithms import VQC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from qiskit.circuit.library import PauliFeatureMap, ZFeatureMap, ZZFeatureMap\n",
    "from qiskit_machine_learning.circuit.library import RawFeatureVector\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_wine\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# Carica il dataset Iris\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Caricamento dei dati\n",
    "data = load_iris()\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "\n",
    "# Preprocessing\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2    , random_state=42, stratify=y)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "print(X_train)\n",
    "\n",
    "\n",
    "# Creazione di un circuito iniziale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlazioni con il target:\n",
      "alcalinity_of_ash               0.517859\n",
      "nonflavanoid_phenols            0.489109\n",
      "malic_acid                      0.437776\n",
      "color_intensity                 0.265668\n",
      "ash                            -0.049643\n",
      "magnesium                      -0.209179\n",
      "alcohol                        -0.328222\n",
      "proanthocyanins                -0.499130\n",
      "hue                            -0.617369\n",
      "proline                        -0.633717\n",
      "total_phenols                  -0.719163\n",
      "od280/od315_of_diluted_wines   -0.788230\n",
      "flavanoids                     -0.847498\n",
      "Name: target, dtype: float64\n",
      "\n",
      "Matrice di correlazione:\n",
      "                               alcohol  malic_acid       ash  \\\n",
      "alcohol                       1.000000    0.094397  0.211545   \n",
      "malic_acid                    0.094397    1.000000  0.164045   \n",
      "ash                           0.211545    0.164045  1.000000   \n",
      "alcalinity_of_ash            -0.310235    0.288500  0.443367   \n",
      "magnesium                     0.270798   -0.054575  0.286587   \n",
      "total_phenols                 0.289101   -0.335167  0.128980   \n",
      "flavanoids                    0.236815   -0.411007  0.115077   \n",
      "nonflavanoid_phenols         -0.155929    0.292977  0.186230   \n",
      "proanthocyanins               0.136698   -0.220746  0.009652   \n",
      "color_intensity               0.546364    0.248985  0.258887   \n",
      "hue                          -0.071747   -0.561296 -0.074667   \n",
      "od280/od315_of_diluted_wines  0.072343   -0.368710  0.003911   \n",
      "proline                       0.643720   -0.192011  0.223626   \n",
      "target                       -0.328222    0.437776 -0.049643   \n",
      "\n",
      "                              alcalinity_of_ash  magnesium  total_phenols  \\\n",
      "alcohol                               -0.310235   0.270798       0.289101   \n",
      "malic_acid                             0.288500  -0.054575      -0.335167   \n",
      "ash                                    0.443367   0.286587       0.128980   \n",
      "alcalinity_of_ash                      1.000000  -0.083333      -0.321113   \n",
      "magnesium                             -0.083333   1.000000       0.214401   \n",
      "total_phenols                         -0.321113   0.214401       1.000000   \n",
      "flavanoids                            -0.351370   0.195784       0.864564   \n",
      "nonflavanoid_phenols                   0.361922  -0.256294      -0.449935   \n",
      "proanthocyanins                       -0.197327   0.236441       0.612413   \n",
      "color_intensity                        0.018732   0.199950      -0.055136   \n",
      "hue                                   -0.273955   0.055398       0.433681   \n",
      "od280/od315_of_diluted_wines          -0.276769   0.066004       0.699949   \n",
      "proline                               -0.440597   0.393351       0.498115   \n",
      "target                                 0.517859  -0.209179      -0.719163   \n",
      "\n",
      "                              flavanoids  nonflavanoid_phenols  \\\n",
      "alcohol                         0.236815             -0.155929   \n",
      "malic_acid                     -0.411007              0.292977   \n",
      "ash                             0.115077              0.186230   \n",
      "alcalinity_of_ash              -0.351370              0.361922   \n",
      "magnesium                       0.195784             -0.256294   \n",
      "total_phenols                   0.864564             -0.449935   \n",
      "flavanoids                      1.000000             -0.537900   \n",
      "nonflavanoid_phenols           -0.537900              1.000000   \n",
      "proanthocyanins                 0.652692             -0.365845   \n",
      "color_intensity                -0.172379              0.139057   \n",
      "hue                             0.543479             -0.262640   \n",
      "od280/od315_of_diluted_wines    0.787194             -0.503270   \n",
      "proline                         0.494193             -0.311385   \n",
      "target                         -0.847498              0.489109   \n",
      "\n",
      "                              proanthocyanins  color_intensity       hue  \\\n",
      "alcohol                              0.136698         0.546364 -0.071747   \n",
      "malic_acid                          -0.220746         0.248985 -0.561296   \n",
      "ash                                  0.009652         0.258887 -0.074667   \n",
      "alcalinity_of_ash                   -0.197327         0.018732 -0.273955   \n",
      "magnesium                            0.236441         0.199950  0.055398   \n",
      "total_phenols                        0.612413        -0.055136  0.433681   \n",
      "flavanoids                           0.652692        -0.172379  0.543479   \n",
      "nonflavanoid_phenols                -0.365845         0.139057 -0.262640   \n",
      "proanthocyanins                      1.000000        -0.025250  0.295544   \n",
      "color_intensity                     -0.025250         1.000000 -0.521813   \n",
      "hue                                  0.295544        -0.521813  1.000000   \n",
      "od280/od315_of_diluted_wines         0.519067        -0.428815  0.565468   \n",
      "proline                              0.330417         0.316100  0.236183   \n",
      "target                              -0.499130         0.265668 -0.617369   \n",
      "\n",
      "                              od280/od315_of_diluted_wines   proline    target  \n",
      "alcohol                                           0.072343  0.643720 -0.328222  \n",
      "malic_acid                                       -0.368710 -0.192011  0.437776  \n",
      "ash                                               0.003911  0.223626 -0.049643  \n",
      "alcalinity_of_ash                                -0.276769 -0.440597  0.517859  \n",
      "magnesium                                         0.066004  0.393351 -0.209179  \n",
      "total_phenols                                     0.699949  0.498115 -0.719163  \n",
      "flavanoids                                        0.787194  0.494193 -0.847498  \n",
      "nonflavanoid_phenols                             -0.503270 -0.311385  0.489109  \n",
      "proanthocyanins                                   0.519067  0.330417 -0.499130  \n",
      "color_intensity                                  -0.428815  0.316100  0.265668  \n",
      "hue                                               0.565468  0.236183 -0.617369  \n",
      "od280/od315_of_diluted_wines                      1.000000  0.312761 -0.788230  \n",
      "proline                                           0.312761  1.000000 -0.633717  \n",
      "target                                           -0.788230 -0.633717  1.000000  \n",
      "\n",
      "DataFrame dopo il drop:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# Carica il dataset Wine\n",
    "wine_data = load_wine()\n",
    "X = pd.DataFrame(wine_data.data, columns=wine_data.feature_names)\n",
    "y = wine_data.target\n",
    "\n",
    "# Aggiungi il target come colonna al DataFrame delle feature\n",
    "X['target'] = y\n",
    "\n",
    "# Calcola la correlazione tra le feature e il target\n",
    "correlation_matrix = X.corr()\n",
    "\n",
    "# Seleziona le correlazioni con il target\n",
    "correlation_with_target = correlation_matrix['target'].drop('target')\n",
    "\n",
    "# Ordina le correlazioni in ordine decrescente\n",
    "correlation_with_target_sorted = correlation_with_target.sort_values(ascending=False)\n",
    "\n",
    "# Stampa le correlazioni con il target\n",
    "print(\"Correlazioni con il target:\")\n",
    "print(correlation_with_target_sorted)\n",
    "\n",
    "# Identifica le colonne da droppare\n",
    "# Seleziona le feature con correlazione compresa tra 0.5 e -0.4\n",
    "columns_to_drop = ['malic_acid', 'color_intensity', 'ash','magnesium','alcohol','proanthocyanins','hue']\n",
    "\n",
    "# Droppa le colonne dal DataFrame\n",
    "df_dropped = X.drop(columns=columns_to_drop,inplace=True)\n",
    "\n",
    "print(\"\\nMatrice di correlazione:\")\n",
    "print(correlation_matrix)\n",
    "print(\"\\nDataFrame dopo il drop:\")\n",
    "print(df_dropped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (142, 7) (142,)\n",
      "Test set shape: (36, 7) (36,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n",
    "# Verifica la divisione\n",
    "print(\"Train set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4) (120,) (30, 4) (30,)\n",
      "[[0.02777778 0.375      0.06779661 0.04166667]\n",
      " [0.16666667 0.20833333 0.59322034 0.66666667]\n",
      " [0.69444444 0.33333333 0.6440678  0.54166667]\n",
      " [0.16666667 0.45833333 0.08474576 0.        ]\n",
      " [0.33333333 0.20833333 0.50847458 0.5       ]\n",
      " [0.55555556 0.20833333 0.6779661  0.75      ]\n",
      " [0.36111111 0.29166667 0.54237288 0.5       ]\n",
      " [0.55555556 0.33333333 0.69491525 0.58333333]\n",
      " [0.94444444 0.41666667 0.86440678 0.91666667]\n",
      " [0.94444444 0.75       0.96610169 0.875     ]\n",
      " [0.91666667 0.41666667 0.94915254 0.83333333]\n",
      " [0.47222222 0.375      0.59322034 0.58333333]\n",
      " [0.19444444 0.         0.42372881 0.375     ]\n",
      " [0.41666667 0.29166667 0.52542373 0.375     ]\n",
      " [0.41666667 0.25       0.50847458 0.45833333]\n",
      " [0.11111111 0.5        0.10169492 0.04166667]\n",
      " [0.22222222 0.75       0.08474576 0.08333333]\n",
      " [0.66666667 0.20833333 0.81355932 0.70833333]\n",
      " [0.58333333 0.45833333 0.76271186 0.70833333]\n",
      " [0.30555556 0.58333333 0.08474576 0.125     ]\n",
      " [0.55555556 0.20833333 0.66101695 0.58333333]\n",
      " [0.22222222 0.75       0.10169492 0.04166667]\n",
      " [1.         0.75       0.91525424 0.79166667]\n",
      " [0.19444444 0.41666667 0.10169492 0.04166667]\n",
      " [0.33333333 0.16666667 0.45762712 0.375     ]\n",
      " [0.55555556 0.375      0.77966102 0.70833333]\n",
      " [0.58333333 0.29166667 0.72881356 0.75      ]\n",
      " [0.19444444 0.58333333 0.10169492 0.125     ]\n",
      " [0.36111111 0.33333333 0.66101695 0.79166667]\n",
      " [0.19444444 0.625      0.05084746 0.08333333]\n",
      " [0.22222222 0.70833333 0.08474576 0.125     ]\n",
      " [0.5        0.33333333 0.50847458 0.5       ]\n",
      " [0.5        0.41666667 0.61016949 0.54166667]\n",
      " [0.38888889 1.         0.08474576 0.125     ]\n",
      " [0.72222222 0.45833333 0.74576271 0.83333333]\n",
      " [0.52777778 0.58333333 0.74576271 0.91666667]\n",
      " [0.38888889 0.25       0.42372881 0.375     ]\n",
      " [0.25       0.29166667 0.49152542 0.54166667]\n",
      " [0.66666667 0.54166667 0.79661017 1.        ]\n",
      " [0.44444444 0.5        0.6440678  0.70833333]\n",
      " [0.08333333 0.58333333 0.06779661 0.08333333]\n",
      " [0.75       0.5        0.62711864 0.54166667]\n",
      " [0.30555556 0.79166667 0.05084746 0.125     ]\n",
      " [0.86111111 0.33333333 0.86440678 0.75      ]\n",
      " [0.16666667 0.41666667 0.06779661 0.04166667]\n",
      " [0.22222222 0.625      0.06779661 0.08333333]\n",
      " [0.41666667 0.29166667 0.69491525 0.75      ]\n",
      " [0.19444444 0.5        0.03389831 0.04166667]\n",
      " [0.05555556 0.125      0.05084746 0.08333333]\n",
      " [0.13888889 0.41666667 0.06779661 0.        ]\n",
      " [0.13888889 0.41666667 0.06779661 0.08333333]\n",
      " [0.36111111 0.375      0.44067797 0.5       ]\n",
      " [0.55555556 0.54166667 0.84745763 1.        ]\n",
      " [0.38888889 0.33333333 0.59322034 0.5       ]\n",
      " [0.22222222 0.625      0.06779661 0.04166667]\n",
      " [0.44444444 0.41666667 0.69491525 0.70833333]\n",
      " [0.38888889 0.33333333 0.52542373 0.5       ]\n",
      " [0.55555556 0.58333333 0.77966102 0.95833333]\n",
      " [0.08333333 0.5        0.06779661 0.04166667]\n",
      " [0.41666667 0.29166667 0.69491525 0.75      ]\n",
      " [0.33333333 0.625      0.05084746 0.04166667]\n",
      " [0.33333333 0.125      0.50847458 0.5       ]\n",
      " [0.38888889 0.20833333 0.6779661  0.79166667]\n",
      " [0.13888889 0.58333333 0.15254237 0.04166667]\n",
      " [0.47222222 0.29166667 0.69491525 0.625     ]\n",
      " [0.33333333 0.25       0.57627119 0.45833333]\n",
      " [0.94444444 0.33333333 0.96610169 0.79166667]\n",
      " [0.36111111 0.41666667 0.59322034 0.58333333]\n",
      " [0.61111111 0.33333333 0.61016949 0.58333333]\n",
      " [0.58333333 0.5        0.72881356 0.91666667]\n",
      " [0.22222222 0.58333333 0.08474576 0.04166667]\n",
      " [0.22222222 0.54166667 0.11864407 0.16666667]\n",
      " [0.19444444 0.66666667 0.06779661 0.04166667]\n",
      " [0.47222222 0.08333333 0.6779661  0.58333333]\n",
      " [0.41666667 0.29166667 0.49152542 0.45833333]\n",
      " [0.80555556 0.41666667 0.81355932 0.625     ]\n",
      " [0.33333333 0.16666667 0.47457627 0.41666667]\n",
      " [0.66666667 0.41666667 0.71186441 0.91666667]\n",
      " [0.55555556 0.29166667 0.66101695 0.70833333]\n",
      " [0.47222222 0.58333333 0.59322034 0.625     ]\n",
      " [0.16666667 0.45833333 0.08474576 0.04166667]\n",
      " [0.41666667 0.33333333 0.69491525 0.95833333]\n",
      " [0.52777778 0.08333333 0.59322034 0.58333333]\n",
      " [0.19444444 0.625      0.10169492 0.20833333]\n",
      " [0.72222222 0.5        0.79661017 0.91666667]\n",
      " [0.25       0.875      0.08474576 0.        ]\n",
      " [0.80555556 0.66666667 0.86440678 1.        ]\n",
      " [0.22222222 0.20833333 0.33898305 0.41666667]\n",
      " [0.66666667 0.45833333 0.62711864 0.58333333]\n",
      " [0.13888889 0.45833333 0.10169492 0.04166667]\n",
      " [0.52777778 0.375      0.55932203 0.5       ]\n",
      " [0.61111111 0.5        0.69491525 0.79166667]\n",
      " [0.22222222 0.75       0.15254237 0.125     ]\n",
      " [0.         0.41666667 0.01694915 0.        ]\n",
      " [0.66666667 0.54166667 0.79661017 0.83333333]\n",
      " [0.94444444 0.25       1.         0.91666667]\n",
      " [0.69444444 0.41666667 0.76271186 0.83333333]\n",
      " [0.55555556 0.125      0.57627119 0.5       ]\n",
      " [0.52777778 0.33333333 0.6440678  0.70833333]\n",
      " [0.30555556 0.79166667 0.11864407 0.125     ]\n",
      " [0.69444444 0.5        0.83050847 0.91666667]\n",
      " [0.38888889 0.375      0.54237288 0.5       ]\n",
      " [0.80555556 0.5        0.84745763 0.70833333]\n",
      " [0.77777778 0.41666667 0.83050847 0.83333333]\n",
      " [0.27777778 0.70833333 0.08474576 0.04166667]\n",
      " [0.58333333 0.375      0.55932203 0.5       ]\n",
      " [0.5        0.33333333 0.62711864 0.45833333]\n",
      " [0.38888889 0.41666667 0.54237288 0.45833333]\n",
      " [0.36111111 0.41666667 0.52542373 0.5       ]\n",
      " [0.66666667 0.45833333 0.57627119 0.54166667]\n",
      " [0.25       0.625      0.08474576 0.04166667]\n",
      " [0.58333333 0.33333333 0.77966102 0.83333333]\n",
      " [0.47222222 0.08333333 0.50847458 0.375     ]\n",
      " [0.44444444 0.41666667 0.54237288 0.58333333]\n",
      " [0.13888889 0.58333333 0.10169492 0.04166667]\n",
      " [0.16666667 0.66666667 0.06779661 0.        ]\n",
      " [0.11111111 0.5        0.05084746 0.04166667]\n",
      " [0.33333333 0.91666667 0.06779661 0.04166667]\n",
      " [0.72222222 0.45833333 0.66101695 0.58333333]\n",
      " [0.08333333 0.45833333 0.08474576 0.04166667]]\n"
     ]
    }
   ],
   "source": [
    "# Caricamento dei dati\n",
    "data = load_iris()\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "\n",
    "# Preprocessing\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2    , random_state=42, stratify=y)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "print(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124, 4) (124,) (54, 4) (54,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# Carica il dataset Wine\n",
    "wine_data = load_wine()\n",
    "X = wine_data.data\n",
    "y = wine_data.target\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "# PCA di X\n",
    "pca = PCA(n_components=4)\n",
    "X = pca.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3   , random_state=42, stratify=y)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature map dimension: 4\n",
      "     ┌────────────────────────────────────┐\n",
      "q_0: ┤0                                   ├\n",
      "     │                                    │\n",
      "q_1: ┤1                                   ├\n",
      "     │  ZZFeatureMap(x[0],x[1],x[2],x[3]) │\n",
      "q_2: ┤2                                   ├\n",
      "     │                                    │\n",
      "q_3: ┤3                                   ├\n",
      "     └────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "num_features = X.shape[1]\n",
    "\n",
    "feature_map = ZZFeatureMap(feature_dimension=num_features, reps=2, entanglement=\"linear\")\n",
    "feature_map.draw(output='mpl')\n",
    "feature_map.decompose().draw(output='mpl')\n",
    "print(\"Feature map dimension:\", feature_map.num_parameters)\n",
    "print(feature_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nimport torch.nn.functional as F\\nimport numpy as np\\n\\nclass QNetwork(nn.Module):\\n    def __init__(self, input_shape, num_actions):\\n        super(QNetwork, self).__init__()\\n        self.fc1 = nn.Linear(input_shape, 128)\\n        self.fc2 = nn.Linear(128, 128)\\n        self.fc3 = nn.Linear(128, num_actions)\\n    \\n    def forward(self, x):\\n        x = F.relu(self.fc1(x))\\n        x = F.relu(self.fc2(x))\\n        return self.fc3(x)\\n\\n# Creazione del modello\\ninput_shape = 4  # Numero di features dell'input (es. dimensioni dell'osservazione)\\nnum_actions = 2  # Numero di azioni possibili\\n\\nmodel = QNetwork(input_shape, num_actions)\\n\\n# Ottimizzatore e perdita\\noptimizer = optim.Adam(model.parameters(), lr=0.001)\\nloss_fn = nn.MSELoss()\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, num_actions)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Creazione del modello\n",
    "input_shape = 4  # Numero di features dell'input (es. dimensioni dell'osservazione)\n",
    "num_actions = 2  # Numero di azioni possibili\n",
    "\n",
    "model = QNetwork(input_shape, num_actions)\n",
    "\n",
    "# Ottimizzatore e perdita\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class DQNAgent:\\n    def __init__(self, model, optimizer, loss_fn, num_actions, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, gamma=0.99):\\n        self.model = model\\n        self.optimizer = optimizer\\n        self.loss_fn = loss_fn\\n        self.num_actions = num_actions\\n        self.epsilon = epsilon\\n        self.epsilon_decay = epsilon_decay\\n        self.epsilon_min = epsilon_min\\n        self.gamma = gamma\\n        self.memory = []\\n        self.batch_size = 32\\n        self.max_memory_size = 1000\\n    \\n    def remember(self, state, action, reward, next_state, done):\\n        if len(self.memory) > self.max_memory_size:\\n            self.memory.pop(0)\\n        self.memory.append((state, action, reward, next_state, done))\\n    \\n    def act(self, state):\\n        if np.random.rand() <= self.epsilon:\\n            return np.random.choice(self.num_actions)\\n        state = torch.FloatTensor(state).unsqueeze(0)\\n        q_values = self.model(state)\\n        return torch.argmax(q_values).item()\\n    \\n    def replay(self):\\n        if len(self.memory) < self.batch_size:\\n            return\\n        \\n        batch = np.random.choice(len(self.memory), self.batch_size, replace=False)\\n        for i in batch:\\n            state, action, reward, next_state, done = self.memory[i]\\n            \\n            state = torch.FloatTensor(state).unsqueeze(0)\\n            next_state = torch.FloatTensor(next_state).unsqueeze(0)\\n            target = reward\\n            \\n            if not done:\\n                target = reward + self.gamma * torch.max(self.model(next_state)).item()\\n            \\n            target_f = self.model(state)\\n            target_f[0][action] = target\\n            \\n            self.optimizer.zero_grad()\\n            output = self.model(state)\\n            loss = self.loss_fn(output, target_f)\\n            loss.backward()\\n            self.optimizer.step()\\n        \\n        if self.epsilon > self.epsilon_min:\\n            self.epsilon *= self.epsilon_decay\\n            \\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"class DQNAgent:\n",
    "    def __init__(self, model, optimizer, loss_fn, num_actions, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, gamma=0.99):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.num_actions = num_actions\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.gamma = gamma\n",
    "        self.memory = []\n",
    "        self.batch_size = 32\n",
    "        self.max_memory_size = 1000\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        if len(self.memory) > self.max_memory_size:\n",
    "            self.memory.pop(0)\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        q_values = self.model(state)\n",
    "        return torch.argmax(q_values).item()\n",
    "    \n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        batch = np.random.choice(len(self.memory), self.batch_size, replace=False)\n",
    "        for i in batch:\n",
    "            state, action, reward, next_state, done = self.memory[i]\n",
    "            \n",
    "            state = torch.FloatTensor(state).unsqueeze(0)\n",
    "            next_state = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "            target = reward\n",
    "            \n",
    "            if not done:\n",
    "                target = reward + self.gamma * torch.max(self.model(next_state)).item()\n",
    "            \n",
    "            target_f = self.model(state)\n",
    "            target_f[0][action] = target\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(state)\n",
    "            loss = self.loss_fn(output, target_f)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "\"\"\"\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport gym\\nimport gym_quantcircuit\\nimport numpy as np\\nimport matplotlib as plt\\n\\nenv = gym.make(\\'quantcircuit-v0\\')\\nagent = DQNAgent(model, optimizer, loss_fn, num_actions)\\n\\nepisodes = 100\\nnum_qubits = 4\\ntest_goal_state = [0j] * (2**num_qubits - 1) + [1+0j]\\nenv.var_init(num_features,\\n             unitary=False,\\n             gate_group=\\'pauli\\',\\n             connectivity=\\'fully_connected\\',\\n             X_train=X_train,\\n             Y_train=y_train,\\n             X_test=X_test,\\n             Y_test=y_test,\\n             feature_map=feature_map,\\n             goal_state=test_goal_state)\\n\\nfor e in range(episodes):\\n    print(f\"Episode {e+1}/{episodes}\")\\n    state = env.reset()\\n    done = False\\n    total_reward = 0\\n    \\n    while not done:\\n        \\n        action = env.sample()\\n        env.gate_list[action]\\n        print(env.gate_list[action])\\n        next_state, reward, done, _ = env.step(action)\\n        \\n        \\n        agent.remember(state, action, reward, next_state, done)\\n        state = next_state\\n        total_reward += reward\\n        \\n        if done:\\n            print(f\"Episode {e+1}/{episodes} - Reward: {total_reward}\")\\n            break\\n    \\n    agent.replay()\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import gym\n",
    "import gym_quantcircuit\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "\n",
    "env = gym.make('quantcircuit-v0')\n",
    "agent = DQNAgent(model, optimizer, loss_fn, num_actions)\n",
    "\n",
    "episodes = 100\n",
    "num_qubits = 4\n",
    "test_goal_state = [0j] * (2**num_qubits - 1) + [1+0j]\n",
    "env.var_init(num_features,\n",
    "             unitary=False,\n",
    "             gate_group='pauli',\n",
    "             connectivity='fully_connected',\n",
    "             X_train=X_train,\n",
    "             Y_train=y_train,\n",
    "             X_test=X_test,\n",
    "             Y_test=y_test,\n",
    "             feature_map=feature_map,\n",
    "             goal_state=test_goal_state)\n",
    "\n",
    "for e in range(episodes):\n",
    "    print(f\"Episode {e+1}/{episodes}\")\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        action = env.sample()\n",
    "        env.gate_list[action]\n",
    "        print(env.gate_list[action])\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        \n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            print(f\"Episode {e+1}/{episodes} - Reward: {total_reward}\")\n",
    "            break\n",
    "    \n",
    "    agent.replay()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_quantcircuit\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "env = gym.make('quantcircuit-v0')\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gate_list:  [((0, 1), <bound method QuantumCircuit.cx of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x151e96f50>>), ((0, 2), <bound method QuantumCircuit.cx of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x151e96f50>>), ((0, 3), <bound method QuantumCircuit.cx of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x151e96f50>>), ((1, 0), <bound method QuantumCircuit.cx of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x151e96f50>>), ((1, 2), <bound method QuantumCircuit.cx of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x151e96f50>>), ((1, 3), <bound method QuantumCircuit.cx of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x151e96f50>>), ((2, 0), <bound method QuantumCircuit.cx of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x151e96f50>>), ((2, 1), <bound method QuantumCircuit.cx of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x151e96f50>>), ((2, 3), <bound method QuantumCircuit.cx of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x151e96f50>>), ((3, 0), <bound method QuantumCircuit.cx of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x151e96f50>>), ((3, 1), <bound method QuantumCircuit.cx of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x151e96f50>>), ((3, 2), <bound method QuantumCircuit.cx of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x151e96f50>>), ((0,), <bound method QuantumCircuit.h of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x151e96f50>>), ((1,), <bound method QuantumCircuit.h of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x151e96f50>>), ((2,), <bound method QuantumCircuit.h of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x151e96f50>>), ((3,), <bound method QuantumCircuit.h of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x151e96f50>>), ((0,), <bound method QuantumCircuit.rx of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x151e96f50>>), ((1,), <bound method QuantumCircuit.rx of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x151e96f50>>), ((2,), <bound method QuantumCircuit.rx of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x151e96f50>>), ((3,), <bound method QuantumCircuit.rx of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x151e96f50>>), ((0,), <bound method QuantumCircuit.ry of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x151e96f50>>), ((1,), <bound method QuantumCircuit.ry of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x151e96f50>>), ((2,), <bound method QuantumCircuit.ry of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x151e96f50>>), ((3,), <bound method QuantumCircuit.ry of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x151e96f50>>), ((0,), <bound method QuantumCircuit.rz of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x151e96f50>>), ((1,), <bound method QuantumCircuit.rz of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x151e96f50>>), ((2,), <bound method QuantumCircuit.rz of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x151e96f50>>), ((3,), <bound method QuantumCircuit.rz of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x151e96f50>>)]\n"
     ]
    }
   ],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 5\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "test_goal_state = [0j] * (2**num_features - 1) + [1+0j]\n",
    "env.var_init(num_qubits=num_features,\n",
    "             unitary=True,\n",
    "             gate_group='clifford',\n",
    "             connectivity='fully_connected',\n",
    "             X_train=X_train,\n",
    "             Y_train=y_train,\n",
    "             X_test=X_test,\n",
    "             Y_test=y_test,\n",
    "             feature_map=feature_map,\n",
    "             goal_state=test_goal_state)\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.get_num_actions()\n",
    "\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = 1\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "\n",
    "    # Assicurati che lo stato sia 2D: (1, num_features)\n",
    "    if len(state.shape) == 1:\n",
    "        state = state.unsqueeze(0)\n",
    "    elif len(state.shape) != 2:\n",
    "        raise ValueError(f\"Forma dello stato inaspettata: {state.shape}\")\n",
    "\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # Stampa la forma dello stato per il debug\n",
    "            print(\"Forma dello stato prima di passare alla policy_net:\", state.shape)\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    print(\"Ottimizzazione del modello\")\n",
    "    \n",
    "    # Extract a batch of transitions from the replay buffer\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    \n",
    "    # Convert the batch of transitions into a batch of arrays (from Transition of batch)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Concatenate the elements of the batch and check dimensions\n",
    "    state_batch = torch.cat(batch.state).view(BATCH_SIZE, -1)\n",
    "\n",
    "    # Ensure actions are scalars and convert them to 1D tensors\n",
    "    action_batch = [a.view(-1) if a.dim() == 0 else a for a in batch.action]\n",
    "    action_batch = torch.stack(action_batch).view(BATCH_SIZE, -1)\n",
    "    \n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    # Calculate Q(s_t, a) using the policy network\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Calculate V(s_{t+1}) for all next states\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), \n",
    "                                  device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    \n",
    "    # Ensure non_final_next_states has the correct shape\n",
    "    if non_final_next_states.dim() == 1:\n",
    "        non_final_next_states = non_final_next_states.unsqueeze(1)\n",
    "    \n",
    "    # Initialize next state values with zeros\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    \n",
    "    # Calculate V(s_{t+1}) for non-terminal states\n",
    "    with torch.no_grad():\n",
    "        if len(non_final_next_states) > 0:\n",
    "            next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "\n",
    "    # Compute expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Calculate Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients to prevent exploding gradients\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    \n",
    "    # Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    print(\"Modello ottimizzato con successo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    print(\"Ottimizzazione del modello\")\n",
    "\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Diagnostica le forme degli stati\n",
    "    for i, state in enumerate(batch.state):\n",
    "        print(f\"State {i} shape: {state.shape} (dim: {state.dim()})\")\n",
    "\n",
    "    # Uniforma le dimensioni degli stati\n",
    "    try:\n",
    "        state_batch = torch.cat([s.unsqueeze(0) if s.dim() == 1 else s for s in batch.state])\n",
    "        print(f\"state_batch shape: {state_batch.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante la concatenazione degli stati: {e}\")\n",
    "        return\n",
    "\n",
    "    # Assicurati che l'azione sia un tensore di tipo long\n",
    "    try:\n",
    "        action_batch = torch.cat([a.view(-1) if a.dim() == 0 else a for a in batch.action]).view(BATCH_SIZE, -1)\n",
    "        print(f\"action_batch shape: {action_batch.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante la concatenazione delle azioni: {e}\")\n",
    "        return\n",
    "\n",
    "    # Assicurati che la ricompensa sia un tensore float\n",
    "    try:\n",
    "        reward_batch = torch.cat(batch.reward).float()\n",
    "        print(f\"reward_batch shape: {reward_batch.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante la concatenazione delle ricompense: {e}\")\n",
    "        return\n",
    "\n",
    "    # Calcola Q(s_t, a) usando la rete policy\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    print(f\"state_action_values shape: {state_action_values.shape}\")\n",
    "\n",
    "    # Crea una maschera per gli stati non finali\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "    # Assicurati che non_final_next_states abbia la forma corretta\n",
    "    if non_final_next_states.dim() == 1:\n",
    "        non_final_next_states = non_final_next_states.unsqueeze(1)\n",
    "\n",
    "    # Inizializza i valori degli stati successivi con zero\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if len(non_final_next_states) > 0:\n",
    "            # Calcola V(s_{t+1}) per stati non terminali\n",
    "            next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "\n",
    "    # Calcola i valori attesi di Q\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Calcola la perdita usando Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Ottimizza il modello\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Clippa i gradienti per prevenire l'esplosione dei gradienti\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "\n",
    "    # Esegui un passo di ottimizzazione\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Perdita: {loss.item()}\")\n",
    "    print(\"Modello ottimizzato con successo!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if torch.cuda.is_available() or torch.backends.mps.is_available():\\n    num_episodes = 50\\nelse:\\n    num_episodes = 50\\nepisode_results=[]\\nfor i_episode in range(num_episodes):\\n    # Initialize the environment and get its state\\n    print(\"Episiodio numero: \"+ str(i_episode+1)+\"/\"+str(num_episodes))\\n    state, info = env.reset()\\n    #print(info.get(\\'current_accuracy\\'))\\n    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\\n    for t in count():\\n        print(state.shape)\\n        action = select_action(state)\\n        observation, reward, terminated, _ = env.step(action)\\n        print(\"La reward sul test set è :\"+str(reward))\\n        print(\"L\\' episiodio risulta essere: \"+str(terminated))\\n        \\n        #reward = torch.tensor([reward], device=device)\\n        reward = torch.tensor([reward], dtype=torch.float32, device=device)\\n\\n        done = terminated\\n\\n        if terminated:\\n            next_state = None\\n        else:\\n            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\\n        print(\"Lo stato :\"+str(state))\\n        print(\"L\\'action è :\"+str(action))\\n        # Store the transition in memory\\n        memory.push(state, action, next_state, reward)\\n\\n        # Move to the next state\\n        state = next_state\\n\\n        # Perform one step of the optimization (on the policy network)\\n        optimize_model()\\n\\n        # Soft update of the target network\\'s weights\\n        # θ′ ← τ θ + (1 −τ )θ′\\n        target_net_state_dict = target_net.state_dict()\\n        policy_net_state_dict = policy_net.state_dict()\\n        for key in policy_net_state_dict:\\n            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\\n        target_net.load_state_dict(target_net_state_dict)\\n        print(\"Pesi aggiornati\")\\n        if done:\\n            episode_durations.append(t + 1)\\n            #plot_durations()\\n            break\\n        # Store the accuracy and rewards for this episode\\n        episode_results.append({\\n            \"episode\": i_episode,\\n            \"final_reward\": reward.item(),  # Convert to a regular float\\n            \"duration\": t + 1,\\n            \"accuracy\": info.get(\\'current_accuracy\\')\\n        })\\n\\n# Print all results after the loop\\nprint(info.get(\\'circuit\\'))\\nfor result in episode_results:\\n    print(f\"Episode {result[\\'episode\\']}: Reward: {result[\\'final_reward\\']}, Duration: {result[\\'duration\\']}, Accuracy: {result[\\'accuracy\\']}\")\\n\\n\\nprint(\\'Complete\\')\\n#plot_durations(show_result=True)\\n#plt.ioff()\\n#plt.show()\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"if torch.cuda.is_available() or torch.backends.mps.is_available():\n",
    "    num_episodes = 50\n",
    "else:\n",
    "    num_episodes = 50\n",
    "episode_results=[]\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    print(\"Episiodio numero: \"+ str(i_episode+1)+\"/\"+str(num_episodes))\n",
    "    state, info = env.reset()\n",
    "    #print(info.get('current_accuracy'))\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        print(state.shape)\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, _ = env.step(action)\n",
    "        print(\"La reward sul test set è :\"+str(reward))\n",
    "        print(\"L' episiodio risulta essere: \"+str(terminated))\n",
    "        \n",
    "        #reward = torch.tensor([reward], device=device)\n",
    "        reward = torch.tensor([reward], dtype=torch.float32, device=device)\n",
    "\n",
    "        done = terminated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        print(\"Lo stato :\"+str(state))\n",
    "        print(\"L'action è :\"+str(action))\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "        print(\"Pesi aggiornati\")\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            #plot_durations()\n",
    "            break\n",
    "        # Store the accuracy and rewards for this episode\n",
    "        episode_results.append({\n",
    "            \"episode\": i_episode,\n",
    "            \"final_reward\": reward.item(),  # Convert to a regular float\n",
    "            \"duration\": t + 1,\n",
    "            \"accuracy\": info.get('current_accuracy')\n",
    "        })\n",
    "\n",
    "# Print all results after the loop\n",
    "print(info.get('circuit'))\n",
    "for result in episode_results:\n",
    "    print(f\"Episode {result['episode']}: Reward: {result['final_reward']}, Duration: {result['duration']}, Accuracy: {result['accuracy']}\")\n",
    "\n",
    "\n",
    "print('Complete')\n",
    "#plot_durations(show_result=True)\n",
    "#plt.ioff()\n",
    "#plt.show()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio numero: 1/15\n",
      "Forma dello stato: torch.Size([1])\n",
      "l'azione schelta è:9\n",
      "Current Quantum Circuit:\n",
      "     \n",
      "q_0: \n",
      "     \n",
      "q_1: \n",
      "     \n",
      "q_2: \n",
      "     \n",
      "q_3: \n",
      "     \n",
      "[0 1 0 0 0 0 2 1 1 2 1 1 2 1 0 2 1 0 2 2 1 2 2 2 1 2 0 1 0 1 0 1 2 1 1 2 1\n",
      " 1 1 0 2 0 0 0 0 1 1 0 2 0 1 1 2 0]\n",
      "Quantum Circuit:\n",
      "     ┌───┐\n",
      "q_0: ┤ X ├\n",
      "     └─┬─┘\n",
      "q_1: ──┼──\n",
      "       │  \n",
      "q_2: ──┼──\n",
      "       │  \n",
      "q_3: ──■──\n",
      "          \n",
      "FEATURE MAP:\n",
      "      ┌──────────────┐┌────────────────────────────────────┐\n",
      "q2_0: ┤0             ├┤0                                   ├\n",
      "      │              ││                                    │\n",
      "q2_1: ┤1             ├┤1                                   ├\n",
      "      │  circuit-202 ││  ZZFeatureMap(x[0],x[1],x[2],x[3]) │\n",
      "q2_2: ┤2             ├┤2                                   ├\n",
      "      │              ││                                    │\n",
      "q2_3: ┤3             ├┤3                                   ├\n",
      "      └──────────────┘└────────────────────────────────────┘\n",
      "Cross-validation scores: [0.8  0.8  0.72 0.76 0.75]\n",
      "Average score: 0.766\n",
      "QSVC classification train score: 0.9032258064516129\n",
      "QSVC classification test score: 0.7407407407407407\n",
      "Prima itrazione:0.9032258064516129\n",
      "Reward: 0.999997387600009\n",
      "La reward sul test set è: 0.999997387600009\n",
      "L'episodio risulta essere: False\n",
      "Lo stato: tensor([[1.]], device='mps:0')\n",
      "L'azione è: tensor([[9]], device='mps:0')\n",
      "Pesi aggiornati\n",
      "Forma dello stato: torch.Size([1])\n",
      "l'azione schelta è:4\n",
      "Current Quantum Circuit:\n",
      "     ┌───┐\n",
      "q_0: ┤ X ├\n",
      "     └─┬─┘\n",
      "q_1: ──┼──\n",
      "       │  \n",
      "q_2: ──┼──\n",
      "       │  \n",
      "q_3: ──■──\n",
      "          \n",
      "[0 1 0 0 0 0 2 1 1 2 1 1 2 1 0 2 1 0 2 2 1 2 2 2 1 2 0 1 0 1 0 1 2 1 1 2 1\n",
      " 1 1 0 2 0 0 0 0 1 1 0 2 0 1 1 2 0]\n",
      "Quantum Circuit:\n",
      "     ┌───┐     \n",
      "q_0: ┤ X ├─────\n",
      "     └─┬─┘     \n",
      "q_1: ──┼────■──\n",
      "       │  ┌─┴─┐\n",
      "q_2: ──┼──┤ X ├\n",
      "       │  └───┘\n",
      "q_3: ──■───────\n",
      "               \n",
      "FEATURE MAP:\n",
      "      ┌──────────────┐┌────────────────────────────────────┐\n",
      "q2_0: ┤0             ├┤0                                   ├\n",
      "      │              ││                                    │\n",
      "q2_1: ┤1             ├┤1                                   ├\n",
      "      │  circuit-202 ││  ZZFeatureMap(x[0],x[1],x[2],x[3]) │\n",
      "q2_2: ┤2             ├┤2                                   ├\n",
      "      │              ││                                    │\n",
      "q2_3: ┤3             ├┤3                                   ├\n",
      "      └──────────────┘└────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "\"\"\"import torch\n",
    "from itertools import count\n",
    "\n",
    "if torch.cuda.is_available() or torch.backends.mps.is_available():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "num_episodes = 50\n",
    "episode_results = []\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    print(\"Episodio numero: \" + str(i_episode + 1) + \"/\" + str(num_episodes))\n",
    "    state, info = env.reset()\n",
    "    print(\"lo stato prima del tensore è: \" + str(state))\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    \n",
    "    for t in count():\n",
    "        print(state.shape)\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, _ = env.step(action)\n",
    "        print(\"La reward sul test set è: \" + str(reward))\n",
    "        print(\"L'episodio risulta essere: \" + str(terminated))\n",
    "        \n",
    "        reward = torch.tensor([reward], dtype=torch.float32, device=device)\n",
    "        done = terminated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "        print(\"Lo stato: \" + str(state))\n",
    "        print(\"L'action è: \" + str(action))\n",
    "        \n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 − τ)θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key] * TAU + target_net_state_dict[key] * (1 - TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "        print(\"Pesi aggiornati\")\n",
    "        \n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            break\n",
    "        \n",
    "        # Store the accuracy and rewards for this episode\n",
    "        episode_results.append({\n",
    "            \"episode\": i_episode,\n",
    "            \"final_reward\": reward.item(),  # Convert to a regular float\n",
    "            \"duration\": t + 1,\n",
    "            \"accuracy\": info.get('current_accuracy')\n",
    "        })\n",
    "\n",
    "# Print all results after the loop\n",
    "print(info.get('circuit'))\n",
    "for result in episode_results:\n",
    "    print(f\"Episode {result['episode']}: Reward: {result['final_reward']}, Duration: {result['duration']}, Accuracy: {result['accuracy']}\")\n",
    "\n",
    "print('Complete')\n",
    "\"\"\"\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_built() else \"cpu\")\n",
    "num_episodes = 15\n",
    "episode_results = []\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    print(f\"Episodio numero: {i_episode+1}/{num_episodes}\")\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)  # Aggiungi dimensione batch\n",
    "\n",
    "    for t in count():\n",
    "        print(f\"Forma dello stato: {state.shape}\")\n",
    "        if state.dim() == 1:  # Se è un tensore 1D, espandilo a 2D\n",
    "            state = state.unsqueeze(0)  # Cambia la forma a (1, 813)\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, measures = env.step(action)\n",
    "\n",
    "        print(f\"La reward sul test set è: {reward}\")\n",
    "        print(f\"L'episodio risulta essere: {terminated}\")\n",
    "\n",
    "        # Converti la reward in un tensore float\n",
    "        reward = torch.tensor([reward], dtype=torch.float32, device=device)\n",
    "\n",
    "        done = terminated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        print(f\"Lo stato: {state}\")\n",
    "        print(f\"L'azione è: {action}\")\n",
    "\n",
    "        # Memorizza la transizione nella memoria di replay\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Passa allo stato successivo\n",
    "        state = next_state\n",
    "\n",
    "        # Ottimizza il modello\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update della rete target\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key] * TAU + target_net_state_dict[key] * (1 - TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "        print(\"Pesi aggiornati\")\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            #plot_durations()\n",
    "            break\n",
    "\n",
    "        # Memorizza i risultati dell'episodio\n",
    "        episode_results.append({\n",
    "            \"episode\": i_episode,\n",
    "            \"final_reward\": reward.item(),\n",
    "            \"duration\": t + 1,\n",
    "            \"accuracy\": measures.get('current_accuracy', 0.0)\n",
    "        })\n",
    "\n",
    "# Stampa tutti i risultati dopo il ciclo\n",
    "print(\"Circuito finale:\")\n",
    "print(info.get('circuit'))\n",
    "for result in episode_results:\n",
    "    print(f\"Episodio {result['episode']}: Reward: {result['final_reward']}, Durata: {result['duration']}, Accuracy: {result['accuracy']}\")\n",
    "\n",
    "print('Addestramento Completo')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello salvato con successo!\n"
     ]
    }
   ],
   "source": [
    "# Salva il modello addestrato\n",
    "torch.save({\n",
    "    'policy_net_state_dict': policy_net.state_dict(),\n",
    "    'target_net_state_dict': target_net.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, 'rl-wine.pth')\n",
    "print(\"Modello salvato con successo!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello ricaricato con successo!\n",
      "DQN(\n",
      "  (layer1): Linear(in_features=1, out_features=128, bias=True)\n",
      "  (layer2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (layer3): Linear(in_features=128, out_features=28, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7f/7v0y2r7x71d8fvv9hcm5rtx80000gn/T/ipykernel_48739/3685394206.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('rl-wine.pth')\n"
     ]
    }
   ],
   "source": [
    "# Ricarica il modello salvato\n",
    "checkpoint = torch.load('rl-wine.pth')\n",
    "policy_net.load_state_dict(checkpoint['policy_net_state_dict'])\n",
    "target_net.load_state_dict(checkpoint['target_net_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "print(\"Modello ricaricato con successo!\")\n",
    "print(policy_net)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gate_list:  [((0,), <bound method QuantumCircuit.id of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x155b32a40>>), ((1,), <bound method QuantumCircuit.id of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x155b32a40>>), ((2,), <bound method QuantumCircuit.id of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x155b32a40>>), ((3,), <bound method QuantumCircuit.id of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x155b32a40>>), ((0,), <bound method QuantumCircuit.h of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x155b32a40>>), ((1,), <bound method QuantumCircuit.h of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x155b32a40>>), ((2,), <bound method QuantumCircuit.h of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x155b32a40>>), ((3,), <bound method QuantumCircuit.h of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x155b32a40>>), ((0,), <bound method QuantumCircuit.x of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x155b32a40>>), ((1,), <bound method QuantumCircuit.x of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x155b32a40>>), ((2,), <bound method QuantumCircuit.x of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x155b32a40>>), ((3,), <bound method QuantumCircuit.x of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x155b32a40>>), ((0,), <bound method QuantumCircuit.z of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x155b32a40>>), ((1,), <bound method QuantumCircuit.z of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x155b32a40>>), ((2,), <bound method QuantumCircuit.z of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x155b32a40>>), ((3,), <bound method QuantumCircuit.z of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x155b32a40>>), ((0, 1), <bound method QuantumCircuit.cx of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x155b32a40>>), ((0, 2), <bound method QuantumCircuit.cx of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x155b32a40>>), ((0, 3), <bound method QuantumCircuit.cx of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x155b32a40>>), ((1, 0), <bound method QuantumCircuit.cx of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x155b32a40>>), ((1, 2), <bound method QuantumCircuit.cx of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x155b32a40>>), ((1, 3), <bound method QuantumCircuit.cx of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x155b32a40>>), ((2, 0), <bound method QuantumCircuit.cx of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x155b32a40>>), ((2, 1), <bound method QuantumCircuit.cx of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x155b32a40>>), ((2, 3), <bound method QuantumCircuit.cx of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x155b32a40>>), ((3, 0), <bound method QuantumCircuit.cx of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x155b32a40>>), ((3, 1), <bound method QuantumCircuit.cx of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x155b32a40>>), ((3, 2), <bound method QuantumCircuit.cx of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x155b32a40>>)]\n",
      "Forma dello stato prima di passare alla policy_net: torch.Size([1, 1])\n",
      "Current Quantum Circuit:\n",
      "     \n",
      "q_0: \n",
      "     \n",
      "q_1: \n",
      "     \n",
      "q_2: \n",
      "     \n",
      "q_3: \n",
      "     \n",
      "[0 1 0 0 0 0 2 1 1 2 1 1 2 1 0 2 1 0 2 2 1 2 2 2 1 2 0 1 0 1 0 1 2 1 1 2 1\n",
      " 1 1 0 2 0 0 0 0 1 1 0 2 0 1 1 2 0]\n",
      "Quantum Circuit:\n",
      "          \n",
      "q_0: ─────\n",
      "     ┌───┐\n",
      "q_1: ┤ Z ├\n",
      "     └───┘\n",
      "q_2: ─────\n",
      "          \n",
      "q_3: ─────\n",
      "          \n",
      "FEATURE MAP:\n",
      "      ┌──────────────┐┌────────────────────────────────────┐\n",
      "q4_0: ┤0             ├┤0                                   ├\n",
      "      │              ││                                    │\n",
      "q4_1: ┤1             ├┤1                                   ├\n",
      "      │  circuit-204 ││  ZZFeatureMap(x[0],x[1],x[2],x[3]) │\n",
      "q4_2: ┤2             ├┤2                                   ├\n",
      "      │              ││                                    │\n",
      "q4_3: ┤3             ├┤3                                   ├\n",
      "      └──────────────┘└────────────────────────────────────┘\n",
      "Cross-validation scores: [0.8  0.8  0.72 0.76 0.75]\n",
      "Average score: 0.766\n",
      "QSVC classification train score: 0.9032258064516129\n",
      "QSVC classification test score: 0.7407407407407407\n",
      "Prima itrazione:0.9032258064516129\n",
      "Reward: 0.999997387600009\n",
      "l'azione schelta è:7\n",
      "Current Quantum Circuit:\n",
      "          \n",
      "q_0: ─────\n",
      "     ┌───┐\n",
      "q_1: ┤ Z ├\n",
      "     └───┘\n",
      "q_2: ─────\n",
      "          \n",
      "q_3: ─────\n",
      "          \n",
      "[0 1 0 0 0 0 2 1 1 2 1 1 2 1 0 2 1 0 2 2 1 2 2 2 1 2 0 1 0 1 0 1 2 1 1 2 1\n",
      " 1 1 0 2 0 0 0 0 1 1 0 2 0 1 1 2 0]\n",
      "Quantum Circuit:\n",
      "          \n",
      "q_0: ─────\n",
      "     ┌───┐\n",
      "q_1: ┤ Z ├\n",
      "     └───┘\n",
      "q_2: ─────\n",
      "     ┌───┐\n",
      "q_3: ┤ H ├\n",
      "     └───┘\n",
      "FEATURE MAP:\n",
      "      ┌──────────────┐┌────────────────────────────────────┐\n",
      "q4_0: ┤0             ├┤0                                   ├\n",
      "      │              ││                                    │\n",
      "q4_1: ┤1             ├┤1                                   ├\n",
      "      │  circuit-204 ││  ZZFeatureMap(x[0],x[1],x[2],x[3]) │\n",
      "q4_2: ┤2             ├┤2                                   ├\n",
      "      │              ││                                    │\n",
      "q4_3: ┤3             ├┤3                                   ├\n",
      "      └──────────────┘└────────────────────────────────────┘\n",
      "Cross-validation scores: [0.68 0.68 0.68 0.8  0.75]\n",
      "Average score: 0.718\n",
      "QSVC classification train score: 0.9516129032258065\n",
      "QSVC classification test score: 0.7222222222222222\n",
      "Reward: 0.34776870704032925\n",
      "l'azione schelta è:7\n",
      "Current Quantum Circuit:\n",
      "          \n",
      "q_0: ─────\n",
      "     ┌───┐\n",
      "q_1: ┤ Z ├\n",
      "     └───┘\n",
      "q_2: ─────\n",
      "     ┌───┐\n",
      "q_3: ┤ H ├\n",
      "     └───┘\n",
      "[0 1 0 0 0 0 2 1 1 2 1 1 2 1 0 2 1 0 2 2 1 2 2 2 1 2 0 1 0 1 0 1 2 1 1 2 1\n",
      " 1 1 0 2 0 0 0 0 1 1 0 2 0 1 1 2 0]\n",
      "Quantum Circuit:\n",
      "               \n",
      "q_0: ──────────\n",
      "     ┌───┐     \n",
      "q_1: ┤ Z ├─────\n",
      "     └───┘     \n",
      "q_2: ──────────\n",
      "     ┌───┐┌───┐\n",
      "q_3: ┤ H ├┤ H ├\n",
      "     └───┘└───┘\n",
      "FEATURE MAP:\n",
      "      ┌──────────────┐┌────────────────────────────────────┐\n",
      "q4_0: ┤0             ├┤0                                   ├\n",
      "      │              ││                                    │\n",
      "q4_1: ┤1             ├┤1                                   ├\n",
      "      │  circuit-204 ││  ZZFeatureMap(x[0],x[1],x[2],x[3]) │\n",
      "q4_2: ┤2             ├┤2                                   ├\n",
      "      │              ││                                    │\n",
      "q4_3: ┤3             ├┤3                                   ├\n",
      "      └──────────────┘└────────────────────────────────────┘\n",
      "Cross-validation scores: [0.8  0.8  0.72 0.76 0.75]\n",
      "Average score: 0.766\n",
      "QSVC classification train score: 0.9032258064516129\n",
      "QSVC classification test score: 0.7407407407407407\n",
      "Reward: -0.34776870704032914\n",
      "l'azione schelta è:4\n",
      "Current Quantum Circuit:\n",
      "               \n",
      "q_0: ──────────\n",
      "     ┌───┐     \n",
      "q_1: ┤ Z ├─────\n",
      "     └───┘     \n",
      "q_2: ──────────\n",
      "     ┌───┐┌───┐\n",
      "q_3: ┤ H ├┤ H ├\n",
      "     └───┘└───┘\n",
      "[0 1 0 0 0 0 2 1 1 2 1 1 2 1 0 2 1 0 2 2 1 2 2 2 1 2 0 1 0 1 0 1 2 1 1 2 1\n",
      " 1 1 0 2 0 0 0 0 1 1 0 2 0 1 1 2 0]\n",
      "Quantum Circuit:\n",
      "     ┌───┐     \n",
      "q_0: ┤ H ├─────\n",
      "     ├───┤     \n",
      "q_1: ┤ Z ├─────\n",
      "     └───┘     \n",
      "q_2: ──────────\n",
      "     ┌───┐┌───┐\n",
      "q_3: ┤ H ├┤ H ├\n",
      "     └───┘└───┘\n",
      "FEATURE MAP:\n",
      "      ┌──────────────┐┌────────────────────────────────────┐\n",
      "q4_0: ┤0             ├┤0                                   ├\n",
      "      │              ││                                    │\n",
      "q4_1: ┤1             ├┤1                                   ├\n",
      "      │  circuit-204 ││  ZZFeatureMap(x[0],x[1],x[2],x[3]) │\n",
      "q4_2: ┤2             ├┤2                                   ├\n",
      "      │              ││                                    │\n",
      "q4_3: ┤3             ├┤3                                   ├\n",
      "      └──────────────┘└────────────────────────────────────┘\n",
      "Cross-validation scores: [0.72       0.8        0.68       0.84       0.70833333]\n",
      "Average score: 0.7496666666666667\n",
      "QSVC classification train score: 0.9274193548387096\n",
      "QSVC classification test score: 0.7592592592592593\n",
      "Reward: 0.1794860820761559\n",
      "l'azione schelta è:26\n",
      "Current Quantum Circuit:\n",
      "     ┌───┐     \n",
      "q_0: ┤ H ├─────\n",
      "     ├───┤     \n",
      "q_1: ┤ Z ├─────\n",
      "     └───┘     \n",
      "q_2: ──────────\n",
      "     ┌───┐┌───┐\n",
      "q_3: ┤ H ├┤ H ├\n",
      "     └───┘└───┘\n",
      "[0 1 0 0 0 0 2 1 1 2 1 1 2 1 0 2 1 0 2 2 1 2 2 2 1 2 0 1 0 1 0 1 2 1 1 2 1\n",
      " 1 1 0 2 0 0 0 0 1 1 0 2 0 1 1 2 0]\n",
      "Quantum Circuit:\n",
      "     ┌───┐          \n",
      "q_0: ┤ H ├──────────\n",
      "     ├───┤     ┌───┐\n",
      "q_1: ┤ Z ├─────┤ X ├\n",
      "     └───┘     └─┬─┘\n",
      "q_2: ────────────┼──\n",
      "     ┌───┐┌───┐  │  \n",
      "q_3: ┤ H ├┤ H ├──■──\n",
      "     └───┘└───┘     \n",
      "FEATURE MAP:\n",
      "      ┌──────────────┐┌────────────────────────────────────┐\n",
      "q4_0: ┤0             ├┤0                                   ├\n",
      "      │              ││                                    │\n",
      "q4_1: ┤1             ├┤1                                   ├\n",
      "      │  circuit-204 ││  ZZFeatureMap(x[0],x[1],x[2],x[3]) │\n",
      "q4_2: ┤2             ├┤2                                   ├\n",
      "      │              ││                                    │\n",
      "q4_3: ┤3             ├┤3                                   ├\n",
      "      └──────────────┘└────────────────────────────────────┘\n",
      "Cross-validation scores: [0.72       0.8        0.68       0.84       0.70833333]\n",
      "Average score: 0.7496666666666667\n",
      "QSVC classification train score: 0.9274193548387096\n",
      "QSVC classification test score: 0.7592592592592593\n",
      "Reward: 0.0\n",
      "Episodio terminato: limite di passi raggiunto.\n",
      "Circuito quantistico generato con successo!\n"
     ]
    }
   ],
   "source": [
    "def generate_quantum_circuit(agent, env):\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = select_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "# Genera il circuito quantistico\n",
    "test_goal_state = [0j] * (2**num_features - 1) + [1+0j]\n",
    "env.var_init(num_qubits=num_features,\n",
    "             unitary=False,\n",
    "             gate_group='pauli',\n",
    "             connectivity='fully_connected',\n",
    "             X_train=X_train,\n",
    "             Y_train=y_train,\n",
    "             X_test=X_test,\n",
    "             Y_test=y_test,\n",
    "             feature_map=feature_map,\n",
    "             goal_state=test_goal_state)\n",
    "\n",
    "quantum_circuit = generate_quantum_circuit(policy_net, env)\n",
    "print(\"Circuito quantistico generato con successo!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnr_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
