{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4) (120,) (30, 4) (30,)\n",
      "[[0.02777778 0.375      0.06779661 0.04166667]\n",
      " [0.16666667 0.20833333 0.59322034 0.66666667]\n",
      " [0.69444444 0.33333333 0.6440678  0.54166667]\n",
      " [0.16666667 0.45833333 0.08474576 0.        ]\n",
      " [0.33333333 0.20833333 0.50847458 0.5       ]\n",
      " [0.55555556 0.20833333 0.6779661  0.75      ]\n",
      " [0.36111111 0.29166667 0.54237288 0.5       ]\n",
      " [0.55555556 0.33333333 0.69491525 0.58333333]\n",
      " [0.94444444 0.41666667 0.86440678 0.91666667]\n",
      " [0.94444444 0.75       0.96610169 0.875     ]\n",
      " [0.91666667 0.41666667 0.94915254 0.83333333]\n",
      " [0.47222222 0.375      0.59322034 0.58333333]\n",
      " [0.19444444 0.         0.42372881 0.375     ]\n",
      " [0.41666667 0.29166667 0.52542373 0.375     ]\n",
      " [0.41666667 0.25       0.50847458 0.45833333]\n",
      " [0.11111111 0.5        0.10169492 0.04166667]\n",
      " [0.22222222 0.75       0.08474576 0.08333333]\n",
      " [0.66666667 0.20833333 0.81355932 0.70833333]\n",
      " [0.58333333 0.45833333 0.76271186 0.70833333]\n",
      " [0.30555556 0.58333333 0.08474576 0.125     ]\n",
      " [0.55555556 0.20833333 0.66101695 0.58333333]\n",
      " [0.22222222 0.75       0.10169492 0.04166667]\n",
      " [1.         0.75       0.91525424 0.79166667]\n",
      " [0.19444444 0.41666667 0.10169492 0.04166667]\n",
      " [0.33333333 0.16666667 0.45762712 0.375     ]\n",
      " [0.55555556 0.375      0.77966102 0.70833333]\n",
      " [0.58333333 0.29166667 0.72881356 0.75      ]\n",
      " [0.19444444 0.58333333 0.10169492 0.125     ]\n",
      " [0.36111111 0.33333333 0.66101695 0.79166667]\n",
      " [0.19444444 0.625      0.05084746 0.08333333]\n",
      " [0.22222222 0.70833333 0.08474576 0.125     ]\n",
      " [0.5        0.33333333 0.50847458 0.5       ]\n",
      " [0.5        0.41666667 0.61016949 0.54166667]\n",
      " [0.38888889 1.         0.08474576 0.125     ]\n",
      " [0.72222222 0.45833333 0.74576271 0.83333333]\n",
      " [0.52777778 0.58333333 0.74576271 0.91666667]\n",
      " [0.38888889 0.25       0.42372881 0.375     ]\n",
      " [0.25       0.29166667 0.49152542 0.54166667]\n",
      " [0.66666667 0.54166667 0.79661017 1.        ]\n",
      " [0.44444444 0.5        0.6440678  0.70833333]\n",
      " [0.08333333 0.58333333 0.06779661 0.08333333]\n",
      " [0.75       0.5        0.62711864 0.54166667]\n",
      " [0.30555556 0.79166667 0.05084746 0.125     ]\n",
      " [0.86111111 0.33333333 0.86440678 0.75      ]\n",
      " [0.16666667 0.41666667 0.06779661 0.04166667]\n",
      " [0.22222222 0.625      0.06779661 0.08333333]\n",
      " [0.41666667 0.29166667 0.69491525 0.75      ]\n",
      " [0.19444444 0.5        0.03389831 0.04166667]\n",
      " [0.05555556 0.125      0.05084746 0.08333333]\n",
      " [0.13888889 0.41666667 0.06779661 0.        ]\n",
      " [0.13888889 0.41666667 0.06779661 0.08333333]\n",
      " [0.36111111 0.375      0.44067797 0.5       ]\n",
      " [0.55555556 0.54166667 0.84745763 1.        ]\n",
      " [0.38888889 0.33333333 0.59322034 0.5       ]\n",
      " [0.22222222 0.625      0.06779661 0.04166667]\n",
      " [0.44444444 0.41666667 0.69491525 0.70833333]\n",
      " [0.38888889 0.33333333 0.52542373 0.5       ]\n",
      " [0.55555556 0.58333333 0.77966102 0.95833333]\n",
      " [0.08333333 0.5        0.06779661 0.04166667]\n",
      " [0.41666667 0.29166667 0.69491525 0.75      ]\n",
      " [0.33333333 0.625      0.05084746 0.04166667]\n",
      " [0.33333333 0.125      0.50847458 0.5       ]\n",
      " [0.38888889 0.20833333 0.6779661  0.79166667]\n",
      " [0.13888889 0.58333333 0.15254237 0.04166667]\n",
      " [0.47222222 0.29166667 0.69491525 0.625     ]\n",
      " [0.33333333 0.25       0.57627119 0.45833333]\n",
      " [0.94444444 0.33333333 0.96610169 0.79166667]\n",
      " [0.36111111 0.41666667 0.59322034 0.58333333]\n",
      " [0.61111111 0.33333333 0.61016949 0.58333333]\n",
      " [0.58333333 0.5        0.72881356 0.91666667]\n",
      " [0.22222222 0.58333333 0.08474576 0.04166667]\n",
      " [0.22222222 0.54166667 0.11864407 0.16666667]\n",
      " [0.19444444 0.66666667 0.06779661 0.04166667]\n",
      " [0.47222222 0.08333333 0.6779661  0.58333333]\n",
      " [0.41666667 0.29166667 0.49152542 0.45833333]\n",
      " [0.80555556 0.41666667 0.81355932 0.625     ]\n",
      " [0.33333333 0.16666667 0.47457627 0.41666667]\n",
      " [0.66666667 0.41666667 0.71186441 0.91666667]\n",
      " [0.55555556 0.29166667 0.66101695 0.70833333]\n",
      " [0.47222222 0.58333333 0.59322034 0.625     ]\n",
      " [0.16666667 0.45833333 0.08474576 0.04166667]\n",
      " [0.41666667 0.33333333 0.69491525 0.95833333]\n",
      " [0.52777778 0.08333333 0.59322034 0.58333333]\n",
      " [0.19444444 0.625      0.10169492 0.20833333]\n",
      " [0.72222222 0.5        0.79661017 0.91666667]\n",
      " [0.25       0.875      0.08474576 0.        ]\n",
      " [0.80555556 0.66666667 0.86440678 1.        ]\n",
      " [0.22222222 0.20833333 0.33898305 0.41666667]\n",
      " [0.66666667 0.45833333 0.62711864 0.58333333]\n",
      " [0.13888889 0.45833333 0.10169492 0.04166667]\n",
      " [0.52777778 0.375      0.55932203 0.5       ]\n",
      " [0.61111111 0.5        0.69491525 0.79166667]\n",
      " [0.22222222 0.75       0.15254237 0.125     ]\n",
      " [0.         0.41666667 0.01694915 0.        ]\n",
      " [0.66666667 0.54166667 0.79661017 0.83333333]\n",
      " [0.94444444 0.25       1.         0.91666667]\n",
      " [0.69444444 0.41666667 0.76271186 0.83333333]\n",
      " [0.55555556 0.125      0.57627119 0.5       ]\n",
      " [0.52777778 0.33333333 0.6440678  0.70833333]\n",
      " [0.30555556 0.79166667 0.11864407 0.125     ]\n",
      " [0.69444444 0.5        0.83050847 0.91666667]\n",
      " [0.38888889 0.375      0.54237288 0.5       ]\n",
      " [0.80555556 0.5        0.84745763 0.70833333]\n",
      " [0.77777778 0.41666667 0.83050847 0.83333333]\n",
      " [0.27777778 0.70833333 0.08474576 0.04166667]\n",
      " [0.58333333 0.375      0.55932203 0.5       ]\n",
      " [0.5        0.33333333 0.62711864 0.45833333]\n",
      " [0.38888889 0.41666667 0.54237288 0.45833333]\n",
      " [0.36111111 0.41666667 0.52542373 0.5       ]\n",
      " [0.66666667 0.45833333 0.57627119 0.54166667]\n",
      " [0.25       0.625      0.08474576 0.04166667]\n",
      " [0.58333333 0.33333333 0.77966102 0.83333333]\n",
      " [0.47222222 0.08333333 0.50847458 0.375     ]\n",
      " [0.44444444 0.41666667 0.54237288 0.58333333]\n",
      " [0.13888889 0.58333333 0.10169492 0.04166667]\n",
      " [0.16666667 0.66666667 0.06779661 0.        ]\n",
      " [0.11111111 0.5        0.05084746 0.04166667]\n",
      " [0.33333333 0.91666667 0.06779661 0.04166667]\n",
      " [0.72222222 0.45833333 0.66101695 0.58333333]\n",
      " [0.08333333 0.45833333 0.08474576 0.04166667]]\n"
     ]
    }
   ],
   "source": [
    "from qiskit import QuantumCircuit\n",
    "from qiskit.visualization import plot_histogram\n",
    "from qiskit_machine_learning.algorithms import VQC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from qiskit.circuit.library import PauliFeatureMap, ZFeatureMap, ZZFeatureMap\n",
    "from qiskit_machine_learning.circuit.library import RawFeatureVector\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_wine\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# Carica il dataset Iris\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Caricamento dei dati\n",
    "data = load_iris()\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "\n",
    "# Preprocessing\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2    , random_state=42, stratify=y)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "print(X_train)\n",
    "\n",
    "\n",
    "# Creazione di un circuito iniziale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlazioni con il target:\n",
      "alcalinity_of_ash               0.517859\n",
      "nonflavanoid_phenols            0.489109\n",
      "malic_acid                      0.437776\n",
      "color_intensity                 0.265668\n",
      "ash                            -0.049643\n",
      "magnesium                      -0.209179\n",
      "alcohol                        -0.328222\n",
      "proanthocyanins                -0.499130\n",
      "hue                            -0.617369\n",
      "proline                        -0.633717\n",
      "total_phenols                  -0.719163\n",
      "od280/od315_of_diluted_wines   -0.788230\n",
      "flavanoids                     -0.847498\n",
      "Name: target, dtype: float64\n",
      "\n",
      "Matrice di correlazione:\n",
      "                               alcohol  malic_acid       ash  \\\n",
      "alcohol                       1.000000    0.094397  0.211545   \n",
      "malic_acid                    0.094397    1.000000  0.164045   \n",
      "ash                           0.211545    0.164045  1.000000   \n",
      "alcalinity_of_ash            -0.310235    0.288500  0.443367   \n",
      "magnesium                     0.270798   -0.054575  0.286587   \n",
      "total_phenols                 0.289101   -0.335167  0.128980   \n",
      "flavanoids                    0.236815   -0.411007  0.115077   \n",
      "nonflavanoid_phenols         -0.155929    0.292977  0.186230   \n",
      "proanthocyanins               0.136698   -0.220746  0.009652   \n",
      "color_intensity               0.546364    0.248985  0.258887   \n",
      "hue                          -0.071747   -0.561296 -0.074667   \n",
      "od280/od315_of_diluted_wines  0.072343   -0.368710  0.003911   \n",
      "proline                       0.643720   -0.192011  0.223626   \n",
      "target                       -0.328222    0.437776 -0.049643   \n",
      "\n",
      "                              alcalinity_of_ash  magnesium  total_phenols  \\\n",
      "alcohol                               -0.310235   0.270798       0.289101   \n",
      "malic_acid                             0.288500  -0.054575      -0.335167   \n",
      "ash                                    0.443367   0.286587       0.128980   \n",
      "alcalinity_of_ash                      1.000000  -0.083333      -0.321113   \n",
      "magnesium                             -0.083333   1.000000       0.214401   \n",
      "total_phenols                         -0.321113   0.214401       1.000000   \n",
      "flavanoids                            -0.351370   0.195784       0.864564   \n",
      "nonflavanoid_phenols                   0.361922  -0.256294      -0.449935   \n",
      "proanthocyanins                       -0.197327   0.236441       0.612413   \n",
      "color_intensity                        0.018732   0.199950      -0.055136   \n",
      "hue                                   -0.273955   0.055398       0.433681   \n",
      "od280/od315_of_diluted_wines          -0.276769   0.066004       0.699949   \n",
      "proline                               -0.440597   0.393351       0.498115   \n",
      "target                                 0.517859  -0.209179      -0.719163   \n",
      "\n",
      "                              flavanoids  nonflavanoid_phenols  \\\n",
      "alcohol                         0.236815             -0.155929   \n",
      "malic_acid                     -0.411007              0.292977   \n",
      "ash                             0.115077              0.186230   \n",
      "alcalinity_of_ash              -0.351370              0.361922   \n",
      "magnesium                       0.195784             -0.256294   \n",
      "total_phenols                   0.864564             -0.449935   \n",
      "flavanoids                      1.000000             -0.537900   \n",
      "nonflavanoid_phenols           -0.537900              1.000000   \n",
      "proanthocyanins                 0.652692             -0.365845   \n",
      "color_intensity                -0.172379              0.139057   \n",
      "hue                             0.543479             -0.262640   \n",
      "od280/od315_of_diluted_wines    0.787194             -0.503270   \n",
      "proline                         0.494193             -0.311385   \n",
      "target                         -0.847498              0.489109   \n",
      "\n",
      "                              proanthocyanins  color_intensity       hue  \\\n",
      "alcohol                              0.136698         0.546364 -0.071747   \n",
      "malic_acid                          -0.220746         0.248985 -0.561296   \n",
      "ash                                  0.009652         0.258887 -0.074667   \n",
      "alcalinity_of_ash                   -0.197327         0.018732 -0.273955   \n",
      "magnesium                            0.236441         0.199950  0.055398   \n",
      "total_phenols                        0.612413        -0.055136  0.433681   \n",
      "flavanoids                           0.652692        -0.172379  0.543479   \n",
      "nonflavanoid_phenols                -0.365845         0.139057 -0.262640   \n",
      "proanthocyanins                      1.000000        -0.025250  0.295544   \n",
      "color_intensity                     -0.025250         1.000000 -0.521813   \n",
      "hue                                  0.295544        -0.521813  1.000000   \n",
      "od280/od315_of_diluted_wines         0.519067        -0.428815  0.565468   \n",
      "proline                              0.330417         0.316100  0.236183   \n",
      "target                              -0.499130         0.265668 -0.617369   \n",
      "\n",
      "                              od280/od315_of_diluted_wines   proline    target  \n",
      "alcohol                                           0.072343  0.643720 -0.328222  \n",
      "malic_acid                                       -0.368710 -0.192011  0.437776  \n",
      "ash                                               0.003911  0.223626 -0.049643  \n",
      "alcalinity_of_ash                                -0.276769 -0.440597  0.517859  \n",
      "magnesium                                         0.066004  0.393351 -0.209179  \n",
      "total_phenols                                     0.699949  0.498115 -0.719163  \n",
      "flavanoids                                        0.787194  0.494193 -0.847498  \n",
      "nonflavanoid_phenols                             -0.503270 -0.311385  0.489109  \n",
      "proanthocyanins                                   0.519067  0.330417 -0.499130  \n",
      "color_intensity                                  -0.428815  0.316100  0.265668  \n",
      "hue                                               0.565468  0.236183 -0.617369  \n",
      "od280/od315_of_diluted_wines                      1.000000  0.312761 -0.788230  \n",
      "proline                                           0.312761  1.000000 -0.633717  \n",
      "target                                           -0.788230 -0.633717  1.000000  \n",
      "\n",
      "DataFrame dopo il drop:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# Carica il dataset Wine\n",
    "wine_data = load_wine()\n",
    "X = pd.DataFrame(wine_data.data, columns=wine_data.feature_names)\n",
    "y = wine_data.target\n",
    "\n",
    "# Aggiungi il target come colonna al DataFrame delle feature\n",
    "X['target'] = y\n",
    "\n",
    "# Calcola la correlazione tra le feature e il target\n",
    "correlation_matrix = X.corr()\n",
    "\n",
    "# Seleziona le correlazioni con il target\n",
    "correlation_with_target = correlation_matrix['target'].drop('target')\n",
    "\n",
    "# Ordina le correlazioni in ordine decrescente\n",
    "correlation_with_target_sorted = correlation_with_target.sort_values(ascending=False)\n",
    "\n",
    "# Stampa le correlazioni con il target\n",
    "print(\"Correlazioni con il target:\")\n",
    "print(correlation_with_target_sorted)\n",
    "\n",
    "# Identifica le colonne da droppare\n",
    "# Seleziona le feature con correlazione compresa tra 0.5 e -0.4\n",
    "columns_to_drop = ['malic_acid', 'color_intensity', 'ash','magnesium','alcohol','proanthocyanins','hue']\n",
    "\n",
    "# Droppa le colonne dal DataFrame\n",
    "df_dropped = X.drop(columns=columns_to_drop,inplace=True)\n",
    "\n",
    "print(\"\\nMatrice di correlazione:\")\n",
    "print(correlation_matrix)\n",
    "print(\"\\nDataFrame dopo il drop:\")\n",
    "print(df_dropped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (142, 7) (142,)\n",
      "Test set shape: (36, 7) (36,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n",
    "# Verifica la divisione\n",
    "print(\"Train set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4) (120,) (30, 4) (30,)\n",
      "[[0.02777778 0.375      0.06779661 0.04166667]\n",
      " [0.16666667 0.20833333 0.59322034 0.66666667]\n",
      " [0.69444444 0.33333333 0.6440678  0.54166667]\n",
      " [0.16666667 0.45833333 0.08474576 0.        ]\n",
      " [0.33333333 0.20833333 0.50847458 0.5       ]\n",
      " [0.55555556 0.20833333 0.6779661  0.75      ]\n",
      " [0.36111111 0.29166667 0.54237288 0.5       ]\n",
      " [0.55555556 0.33333333 0.69491525 0.58333333]\n",
      " [0.94444444 0.41666667 0.86440678 0.91666667]\n",
      " [0.94444444 0.75       0.96610169 0.875     ]\n",
      " [0.91666667 0.41666667 0.94915254 0.83333333]\n",
      " [0.47222222 0.375      0.59322034 0.58333333]\n",
      " [0.19444444 0.         0.42372881 0.375     ]\n",
      " [0.41666667 0.29166667 0.52542373 0.375     ]\n",
      " [0.41666667 0.25       0.50847458 0.45833333]\n",
      " [0.11111111 0.5        0.10169492 0.04166667]\n",
      " [0.22222222 0.75       0.08474576 0.08333333]\n",
      " [0.66666667 0.20833333 0.81355932 0.70833333]\n",
      " [0.58333333 0.45833333 0.76271186 0.70833333]\n",
      " [0.30555556 0.58333333 0.08474576 0.125     ]\n",
      " [0.55555556 0.20833333 0.66101695 0.58333333]\n",
      " [0.22222222 0.75       0.10169492 0.04166667]\n",
      " [1.         0.75       0.91525424 0.79166667]\n",
      " [0.19444444 0.41666667 0.10169492 0.04166667]\n",
      " [0.33333333 0.16666667 0.45762712 0.375     ]\n",
      " [0.55555556 0.375      0.77966102 0.70833333]\n",
      " [0.58333333 0.29166667 0.72881356 0.75      ]\n",
      " [0.19444444 0.58333333 0.10169492 0.125     ]\n",
      " [0.36111111 0.33333333 0.66101695 0.79166667]\n",
      " [0.19444444 0.625      0.05084746 0.08333333]\n",
      " [0.22222222 0.70833333 0.08474576 0.125     ]\n",
      " [0.5        0.33333333 0.50847458 0.5       ]\n",
      " [0.5        0.41666667 0.61016949 0.54166667]\n",
      " [0.38888889 1.         0.08474576 0.125     ]\n",
      " [0.72222222 0.45833333 0.74576271 0.83333333]\n",
      " [0.52777778 0.58333333 0.74576271 0.91666667]\n",
      " [0.38888889 0.25       0.42372881 0.375     ]\n",
      " [0.25       0.29166667 0.49152542 0.54166667]\n",
      " [0.66666667 0.54166667 0.79661017 1.        ]\n",
      " [0.44444444 0.5        0.6440678  0.70833333]\n",
      " [0.08333333 0.58333333 0.06779661 0.08333333]\n",
      " [0.75       0.5        0.62711864 0.54166667]\n",
      " [0.30555556 0.79166667 0.05084746 0.125     ]\n",
      " [0.86111111 0.33333333 0.86440678 0.75      ]\n",
      " [0.16666667 0.41666667 0.06779661 0.04166667]\n",
      " [0.22222222 0.625      0.06779661 0.08333333]\n",
      " [0.41666667 0.29166667 0.69491525 0.75      ]\n",
      " [0.19444444 0.5        0.03389831 0.04166667]\n",
      " [0.05555556 0.125      0.05084746 0.08333333]\n",
      " [0.13888889 0.41666667 0.06779661 0.        ]\n",
      " [0.13888889 0.41666667 0.06779661 0.08333333]\n",
      " [0.36111111 0.375      0.44067797 0.5       ]\n",
      " [0.55555556 0.54166667 0.84745763 1.        ]\n",
      " [0.38888889 0.33333333 0.59322034 0.5       ]\n",
      " [0.22222222 0.625      0.06779661 0.04166667]\n",
      " [0.44444444 0.41666667 0.69491525 0.70833333]\n",
      " [0.38888889 0.33333333 0.52542373 0.5       ]\n",
      " [0.55555556 0.58333333 0.77966102 0.95833333]\n",
      " [0.08333333 0.5        0.06779661 0.04166667]\n",
      " [0.41666667 0.29166667 0.69491525 0.75      ]\n",
      " [0.33333333 0.625      0.05084746 0.04166667]\n",
      " [0.33333333 0.125      0.50847458 0.5       ]\n",
      " [0.38888889 0.20833333 0.6779661  0.79166667]\n",
      " [0.13888889 0.58333333 0.15254237 0.04166667]\n",
      " [0.47222222 0.29166667 0.69491525 0.625     ]\n",
      " [0.33333333 0.25       0.57627119 0.45833333]\n",
      " [0.94444444 0.33333333 0.96610169 0.79166667]\n",
      " [0.36111111 0.41666667 0.59322034 0.58333333]\n",
      " [0.61111111 0.33333333 0.61016949 0.58333333]\n",
      " [0.58333333 0.5        0.72881356 0.91666667]\n",
      " [0.22222222 0.58333333 0.08474576 0.04166667]\n",
      " [0.22222222 0.54166667 0.11864407 0.16666667]\n",
      " [0.19444444 0.66666667 0.06779661 0.04166667]\n",
      " [0.47222222 0.08333333 0.6779661  0.58333333]\n",
      " [0.41666667 0.29166667 0.49152542 0.45833333]\n",
      " [0.80555556 0.41666667 0.81355932 0.625     ]\n",
      " [0.33333333 0.16666667 0.47457627 0.41666667]\n",
      " [0.66666667 0.41666667 0.71186441 0.91666667]\n",
      " [0.55555556 0.29166667 0.66101695 0.70833333]\n",
      " [0.47222222 0.58333333 0.59322034 0.625     ]\n",
      " [0.16666667 0.45833333 0.08474576 0.04166667]\n",
      " [0.41666667 0.33333333 0.69491525 0.95833333]\n",
      " [0.52777778 0.08333333 0.59322034 0.58333333]\n",
      " [0.19444444 0.625      0.10169492 0.20833333]\n",
      " [0.72222222 0.5        0.79661017 0.91666667]\n",
      " [0.25       0.875      0.08474576 0.        ]\n",
      " [0.80555556 0.66666667 0.86440678 1.        ]\n",
      " [0.22222222 0.20833333 0.33898305 0.41666667]\n",
      " [0.66666667 0.45833333 0.62711864 0.58333333]\n",
      " [0.13888889 0.45833333 0.10169492 0.04166667]\n",
      " [0.52777778 0.375      0.55932203 0.5       ]\n",
      " [0.61111111 0.5        0.69491525 0.79166667]\n",
      " [0.22222222 0.75       0.15254237 0.125     ]\n",
      " [0.         0.41666667 0.01694915 0.        ]\n",
      " [0.66666667 0.54166667 0.79661017 0.83333333]\n",
      " [0.94444444 0.25       1.         0.91666667]\n",
      " [0.69444444 0.41666667 0.76271186 0.83333333]\n",
      " [0.55555556 0.125      0.57627119 0.5       ]\n",
      " [0.52777778 0.33333333 0.6440678  0.70833333]\n",
      " [0.30555556 0.79166667 0.11864407 0.125     ]\n",
      " [0.69444444 0.5        0.83050847 0.91666667]\n",
      " [0.38888889 0.375      0.54237288 0.5       ]\n",
      " [0.80555556 0.5        0.84745763 0.70833333]\n",
      " [0.77777778 0.41666667 0.83050847 0.83333333]\n",
      " [0.27777778 0.70833333 0.08474576 0.04166667]\n",
      " [0.58333333 0.375      0.55932203 0.5       ]\n",
      " [0.5        0.33333333 0.62711864 0.45833333]\n",
      " [0.38888889 0.41666667 0.54237288 0.45833333]\n",
      " [0.36111111 0.41666667 0.52542373 0.5       ]\n",
      " [0.66666667 0.45833333 0.57627119 0.54166667]\n",
      " [0.25       0.625      0.08474576 0.04166667]\n",
      " [0.58333333 0.33333333 0.77966102 0.83333333]\n",
      " [0.47222222 0.08333333 0.50847458 0.375     ]\n",
      " [0.44444444 0.41666667 0.54237288 0.58333333]\n",
      " [0.13888889 0.58333333 0.10169492 0.04166667]\n",
      " [0.16666667 0.66666667 0.06779661 0.        ]\n",
      " [0.11111111 0.5        0.05084746 0.04166667]\n",
      " [0.33333333 0.91666667 0.06779661 0.04166667]\n",
      " [0.72222222 0.45833333 0.66101695 0.58333333]\n",
      " [0.08333333 0.45833333 0.08474576 0.04166667]]\n"
     ]
    }
   ],
   "source": [
    "# Caricamento dei dati\n",
    "data = load_iris()\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "\n",
    "# Preprocessing\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2    , random_state=42, stratify=y)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "print(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124, 4) (124,) (54, 4) (54,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# Carica il dataset Wine\n",
    "wine_data = load_wine()\n",
    "X = wine_data.data\n",
    "y = wine_data.target\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "# PCA di X\n",
    "pca = PCA(n_components=4)\n",
    "X = pca.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3   , random_state=42, stratify=y)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature map dimension: 4\n",
      "     ┌────────────────────────────────────┐\n",
      "q_0: ┤0                                   ├\n",
      "     │                                    │\n",
      "q_1: ┤1                                   ├\n",
      "     │  ZZFeatureMap(x[0],x[1],x[2],x[3]) │\n",
      "q_2: ┤2                                   ├\n",
      "     │                                    │\n",
      "q_3: ┤3                                   ├\n",
      "     └────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "num_features = X.shape[1]\n",
    "\n",
    "feature_map = ZZFeatureMap(feature_dimension=num_features, reps=2, entanglement=\"linear\")\n",
    "feature_map.draw(output='mpl')\n",
    "feature_map.decompose().draw(output='mpl')\n",
    "print(\"Feature map dimension:\", feature_map.num_parameters)\n",
    "print(feature_map)\n",
    "num_qubits=num_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nimport torch.nn.functional as F\\nimport numpy as np\\n\\nclass QNetwork(nn.Module):\\n    def __init__(self, input_shape, num_actions):\\n        super(QNetwork, self).__init__()\\n        self.fc1 = nn.Linear(input_shape, 128)\\n        self.fc2 = nn.Linear(128, 128)\\n        self.fc3 = nn.Linear(128, num_actions)\\n    \\n    def forward(self, x):\\n        x = F.relu(self.fc1(x))\\n        x = F.relu(self.fc2(x))\\n        return self.fc3(x)\\n\\n# Creazione del modello\\ninput_shape = 4  # Numero di features dell'input (es. dimensioni dell'osservazione)\\nnum_actions = 2  # Numero di azioni possibili\\n\\nmodel = QNetwork(input_shape, num_actions)\\n\\n# Ottimizzatore e perdita\\noptimizer = optim.Adam(model.parameters(), lr=0.001)\\nloss_fn = nn.MSELoss()\\n\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, num_actions)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Creazione del modello\n",
    "input_shape = 4  # Numero di features dell'input (es. dimensioni dell'osservazione)\n",
    "num_actions = 2  # Numero di azioni possibili\n",
    "\n",
    "model = QNetwork(input_shape, num_actions)\n",
    "\n",
    "# Ottimizzatore e perdita\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class DQNAgent:\\n    def __init__(self, model, optimizer, loss_fn, num_actions, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, gamma=0.99):\\n        self.model = model\\n        self.optimizer = optimizer\\n        self.loss_fn = loss_fn\\n        self.num_actions = num_actions\\n        self.epsilon = epsilon\\n        self.epsilon_decay = epsilon_decay\\n        self.epsilon_min = epsilon_min\\n        self.gamma = gamma\\n        self.memory = []\\n        self.batch_size = 32\\n        self.max_memory_size = 1000\\n    \\n    def remember(self, state, action, reward, next_state, done):\\n        if len(self.memory) > self.max_memory_size:\\n            self.memory.pop(0)\\n        self.memory.append((state, action, reward, next_state, done))\\n    \\n    def act(self, state):\\n        if np.random.rand() <= self.epsilon:\\n            return np.random.choice(self.num_actions)\\n        state = torch.FloatTensor(state).unsqueeze(0)\\n        q_values = self.model(state)\\n        return torch.argmax(q_values).item()\\n    \\n    def replay(self):\\n        if len(self.memory) < self.batch_size:\\n            return\\n        \\n        batch = np.random.choice(len(self.memory), self.batch_size, replace=False)\\n        for i in batch:\\n            state, action, reward, next_state, done = self.memory[i]\\n            \\n            state = torch.FloatTensor(state).unsqueeze(0)\\n            next_state = torch.FloatTensor(next_state).unsqueeze(0)\\n            target = reward\\n            \\n            if not done:\\n                target = reward + self.gamma * torch.max(self.model(next_state)).item()\\n            \\n            target_f = self.model(state)\\n            target_f[0][action] = target\\n            \\n            self.optimizer.zero_grad()\\n            output = self.model(state)\\n            loss = self.loss_fn(output, target_f)\\n            loss.backward()\\n            self.optimizer.step()\\n        \\n        if self.epsilon > self.epsilon_min:\\n            self.epsilon *= self.epsilon_decay\\n            \\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"class DQNAgent:\n",
    "    def __init__(self, model, optimizer, loss_fn, num_actions, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, gamma=0.99):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.num_actions = num_actions\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.gamma = gamma\n",
    "        self.memory = []\n",
    "        self.batch_size = 32\n",
    "        self.max_memory_size = 1000\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        if len(self.memory) > self.max_memory_size:\n",
    "            self.memory.pop(0)\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        q_values = self.model(state)\n",
    "        return torch.argmax(q_values).item()\n",
    "    \n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        batch = np.random.choice(len(self.memory), self.batch_size, replace=False)\n",
    "        for i in batch:\n",
    "            state, action, reward, next_state, done = self.memory[i]\n",
    "            \n",
    "            state = torch.FloatTensor(state).unsqueeze(0)\n",
    "            next_state = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "            target = reward\n",
    "            \n",
    "            if not done:\n",
    "                target = reward + self.gamma * torch.max(self.model(next_state)).item()\n",
    "            \n",
    "            target_f = self.model(state)\n",
    "            target_f[0][action] = target\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(state)\n",
    "            loss = self.loss_fn(output, target_f)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "\"\"\"\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport gym\\nimport gym_quantcircuit\\nimport numpy as np\\nimport matplotlib as plt\\n\\nenv = gym.make(\\'quantcircuit-v0\\')\\nagent = DQNAgent(model, optimizer, loss_fn, num_actions)\\n\\nepisodes = 100\\nnum_qubits = 4\\ntest_goal_state = [0j] * (2**num_qubits - 1) + [1+0j]\\nenv.var_init(num_features,\\n             unitary=False,\\n             gate_group=\\'pauli\\',\\n             connectivity=\\'fully_connected\\',\\n             X_train=X_train,\\n             Y_train=y_train,\\n             X_test=X_test,\\n             Y_test=y_test,\\n             feature_map=feature_map,\\n             goal_state=test_goal_state)\\n\\nfor e in range(episodes):\\n    print(f\"Episode {e+1}/{episodes}\")\\n    state = env.reset()\\n    done = False\\n    total_reward = 0\\n    \\n    while not done:\\n        \\n        action = env.sample()\\n        env.gate_list[action]\\n        print(env.gate_list[action])\\n        next_state, reward, done, _ = env.step(action)\\n        \\n        \\n        agent.remember(state, action, reward, next_state, done)\\n        state = next_state\\n        total_reward += reward\\n        \\n        if done:\\n            print(f\"Episode {e+1}/{episodes} - Reward: {total_reward}\")\\n            break\\n    \\n    agent.replay()\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import gym\n",
    "import gym_quantcircuit\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "\n",
    "env = gym.make('quantcircuit-v0')\n",
    "agent = DQNAgent(model, optimizer, loss_fn, num_actions)\n",
    "\n",
    "episodes = 100\n",
    "num_qubits = 4\n",
    "test_goal_state = [0j] * (2**num_qubits - 1) + [1+0j]\n",
    "env.var_init(num_features,\n",
    "             unitary=False,\n",
    "             gate_group='pauli',\n",
    "             connectivity='fully_connected',\n",
    "             X_train=X_train,\n",
    "             Y_train=y_train,\n",
    "             X_test=X_test,\n",
    "             Y_test=y_test,\n",
    "             feature_map=feature_map,\n",
    "             goal_state=test_goal_state)\n",
    "\n",
    "for e in range(episodes):\n",
    "    print(f\"Episode {e+1}/{episodes}\")\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        action = env.sample()\n",
    "        env.gate_list[action]\n",
    "        print(env.gate_list[action])\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        \n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            print(f\"Episode {e+1}/{episodes} - Reward: {total_reward}\")\n",
    "            break\n",
    "    \n",
    "    agent.replay()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_quantcircuit\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "env = gym.make('quantcircuit-v0')\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "\n",
    "# Definizione aggiornata della classe Transition\n",
    "Transition = namedtuple('Transition', (\n",
    "    'state', \n",
    "    'action', \n",
    "    'reward', \n",
    "    'next_state', \n",
    "    'done', \n",
    "    'tens_gate_used', \n",
    "    'tens_qubit_used',\n",
    "    'next_tens_gate_used',\n",
    "    'next_tens_qubit_used'\n",
    "))\n",
    "\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "gate_qubit_map = {\n",
    "            'cx': 2,  # CNOT\n",
    "            'cz': 2,  # Controlled-Z\n",
    "            'swap': 2,  # SWAP\n",
    "            'iden': 1,  # Identity\n",
    "            'h': 1,  # Hadamard\n",
    "            'x': 1,  # Pauli-X\n",
    "            'y': 1,  # Pauli-Y\n",
    "            'z': 1,  # Pauli-Z\n",
    "            'rx': 1,  # RX rotation\n",
    "            'ry': 1,  # RY rotation\n",
    "            'rz': 1,  # RZ rotation\n",
    "            # Aggiungi altre porte se necessario\n",
    "        }\n",
    "gate_type_map = {\n",
    "    'cx': 0,     # CNOT\n",
    "    'cz': 1,     # Controlled-Z\n",
    "    'swap': 2,   # SWAP\n",
    "    'iden': 3,   # Identity\n",
    "    'h': 4,      # Hadamard\n",
    "    'x': 5,      # Pauli-X\n",
    "    'y': 6,      # Pauli-Y\n",
    "    'z': 7,      # Pauli-Z\n",
    "    'rx': 8,     # RX rotation\n",
    "    'ry': 9,     # RY rotation\n",
    "    'rz': 10,    # RZ rotation\n",
    "    # Aggiungi altri gate se necessario\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di azioni: 28\n"
     ]
    }
   ],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 1\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "target_update = 10 \n",
    "LR = 1e-4\n",
    "test_goal_state = [0j] * (2**num_features - 1) + [1+0j]\n",
    "env.var_init(num_qubits=num_features,\n",
    "             unitary=True,\n",
    "             gate_group='clifford',\n",
    "             connectivity='fully_connected',\n",
    "             X_train=X_train,\n",
    "             Y_train=y_train,\n",
    "             X_test=X_test,\n",
    "             Y_test=y_test,\n",
    "             feature_map=feature_map,\n",
    "             goal_state=test_goal_state)\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.get_num_actions()\n",
    "print(\"Numero di azioni:\", n_actions)\n",
    "\n",
    "# Get the number of state observations\n",
    "\"\"\"\n",
    "state, info = env.reset()\n",
    "n_observations = 1+11+num_features\n",
    "\"\"\"\n",
    "\n",
    "state_dim = 1  # Supponendo che lo stato sia un numero reale\n",
    "input_dim = state_dim + len(gate_type_map) + num_qubits  # 1 + num_gate_types + num_qubits\n",
    "input_dim=16\n",
    "policy_net = DQN(input_dim, n_actions).to(device)\n",
    "target_net = DQN(input_dim, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\"\"\"\n",
    "def select_action(state,tens_gate_used,tens_qubit_used):\n",
    "    print(\"numero azioni:\", env.get_num_actions())\n",
    "    global steps_done\n",
    "    #valore casuale tra 0 e 1\n",
    "    sample = random.random()\n",
    "    #valore di epsilone per la decisione futura da prendere\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "\n",
    "    # Assicurati che lo stato sia 2D: (1, num_features)\n",
    "    if len(state.shape) == 1:\n",
    "        state = state.unsqueeze(0)\n",
    "    elif len(state.shape) != 2:\n",
    "        raise ValueError(f\"Forma dello stato inaspettata: {state.shape}\")\n",
    "    print(\"Forma dello stato:\", state.shape)\n",
    "    \n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # Stampa la forma dello stato per il debug\n",
    "            print(\"Forma dello stato prima di passare alla policy_net:\", state.shape)\n",
    "            return policy_net(state,tens_gate_used,tens_qubit_used).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.sample()]], device=device, dtype=torch.long)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "def select_action(state, policy_net, n_actions, device, eps_threshold=0.05):\n",
    "    \"\"\"\n",
    "    Seleziona un'azione utilizzando una politica epsilon-greedy.\n",
    "    \n",
    "    Args:\n",
    "        state (torch.Tensor): Tensore dello stato corrente.\n",
    "        policy_net (nn.Module): Rete neurale policy.\n",
    "        n_actions (int): Numero di azioni disponibili.\n",
    "        device (torch.device): Dispositivo PyTorch.\n",
    "        eps_threshold (float): Soglia epsilon per l'esplorazione.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Tensore dell'azione selezionata.\n",
    "    \"\"\"\n",
    "    sample = random.random()\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_state(state):\n",
    "    \"\"\"\n",
    "    Funzione di preprocessamento dello stato.\n",
    "    Implementa la logica necessaria per trasformare lo stato in un tensore.\n",
    "    \"\"\"\n",
    "    # Implementa il preprocessamento specifico del tuo ambiente\n",
    "    # Ad esempio, se lo stato è un numero reale:\n",
    "    real_number = torch.tensor([state], dtype=torch.float32)\n",
    "    return real_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    print(\"Ottimizzazione del modello\")\n",
    "\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Diagnostica le forme degli stati\n",
    "    for i, state in enumerate(batch.state):\n",
    "        print(f\"State {i} shape: {state.shape} (dim: {state.dim()})\")\n",
    "\n",
    "    # Uniforma le dimensioni degli stati\n",
    "    try:\n",
    "        state_batch = torch.cat([s.unsqueeze(0) if s.dim() == 1 else s for s in batch.state])\n",
    "        print(f\"state_batch shape: {state_batch.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante la concatenazione degli stati: {e}\")\n",
    "        return\n",
    "\n",
    "    # Assicurati che l'azione sia un tensore di tipo long\n",
    "    try:\n",
    "        action_batch = torch.cat([a.view(-1) if a.dim() == 0 else a for a in batch.action]).view(BATCH_SIZE, -1)\n",
    "        print(f\"action_batch shape: {action_batch.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante la concatenazione delle azioni: {e}\")\n",
    "        return\n",
    "\n",
    "    # Assicurati che la ricompensa sia un tensore float\n",
    "    try:\n",
    "        reward_batch = torch.cat(batch.reward).float()\n",
    "        print(f\"reward_batch shape: {reward_batch.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante la concatenazione delle ricompense: {e}\")\n",
    "        return\n",
    "\n",
    "    # Calcola Q(s_t, a) usando la rete policy\n",
    "    \n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    print(f\"state_action_values shape: {state_action_values.shape}\")\n",
    "\n",
    "    # Crea una maschera per gli stati non finali\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "    # Assicurati che non_final_next_states abbia la forma corretta\n",
    "    if non_final_next_states.dim() == 1:\n",
    "        non_final_next_states = non_final_next_states.unsqueeze(1)\n",
    "\n",
    "    # Inizializza i valori degli stati successivi con zero\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if len(non_final_next_states) > 0:\n",
    "            # Calcola V(s_{t+1}) per stati non terminali\n",
    "            next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "\n",
    "    # Calcola i valori attesi di Q\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Calcola la perdita usando Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Ottimizza il modello\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Clippa i gradienti per prevenire l'esplosione dei gradienti\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "\n",
    "    # Esegui un passo di ottimizzazione\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Perdita: {loss.item()}\")\n",
    "    print(\"Modello ottimizzato con successo!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode_gate_info(gate_names, device):\n",
    "    \"\"\"\n",
    "    Codifica i nomi dei gate in vettori one-hot basati su gate_type_map.\n",
    "    \n",
    "    Args:\n",
    "        gate_names (list of str): Lista dei nomi dei gate.\n",
    "        device (torch.device): Dispositivo PyTorch.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Tensore one-hot di dimensione [1, num_gate_types].\n",
    "    \"\"\"\n",
    "    num_gate_types = len(gate_type_map)\n",
    "    gate_tensor = torch.zeros(1, num_gate_types, dtype=torch.float32, device=device)\n",
    "    \n",
    "    for gate in gate_names:\n",
    "        if gate in gate_type_map:\n",
    "            gate_index = gate_type_map[gate]\n",
    "            gate_tensor[0][gate_index] = 1  \n",
    "        else:\n",
    "            print(f\"Gate {gate} non trovato nella mappa gate_type_map\")\n",
    "    \n",
    "    return gate_tensor\n",
    "\n",
    "def encode_qubit_info(qubit_info, num_qubits, device):\n",
    "    \"\"\"\n",
    "    Codifica le informazioni sui qubit coinvolti in vettori one-hot.\n",
    "    \n",
    "    Args:\n",
    "        qubit_info (list of int): Lista degli indici dei qubit coinvolti.\n",
    "        num_qubits (int): Numero totale di qubit nel circuito.\n",
    "        device (torch.device): Dispositivo PyTorch.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Tensore one-hot di dimensione [1, num_qubits].\n",
    "    \"\"\"\n",
    "    qubit_tensor = torch.zeros(1, num_qubits, dtype=torch.float32, device=device)\n",
    "    \n",
    "    for qubit in qubit_info:\n",
    "        if isinstance(qubit, torch.Tensor):\n",
    "            qubit = qubit.item()  # Converti il tensore in un valore scalare\n",
    "        qubit_tensor[0][int(qubit)] = 1  # Imposta il valore per i qubit attivi\n",
    "    \n",
    "    return qubit_tensor\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "# Definizione della mappa dei tipi di gate\n",
    "gate_type_map = {\n",
    "    'cx': 0,     # CNOT\n",
    "    'cz': 1,     # Controlled-Z\n",
    "    'swap': 2,   # SWAP\n",
    "    'iden': 3,   # Identity\n",
    "    'h': 4,      # Hadamard\n",
    "    'x': 5,      # Pauli-X\n",
    "    'y': 6,      # Pauli-Y\n",
    "    'z': 7,      # Pauli-Z\n",
    "    'rx': 8,     # RX rotation\n",
    "    'ry': 9,     # RY rotation\n",
    "    'rz': 10,    # RZ rotation\n",
    "    # Aggiungi altri gate se necessario\n",
    "}\n",
    "\n",
    "# Mappa dei gate ai qubit coinvolti\n",
    "gate_qubit_map = {\n",
    "    'cx': 2,\n",
    "    'cz': 2,\n",
    "    'swap': 2,\n",
    "    'iden': 1,\n",
    "    'h': 1,\n",
    "    'x': 1,\n",
    "    'y': 1,\n",
    "    'z': 1,\n",
    "    'rx': 1,\n",
    "    'ry': 1,\n",
    "    'rz': 1,\n",
    "    # Aggiungi altre porte se necessario\n",
    "}\n",
    "\n",
    "# Funzione per codificare le informazioni sui gate\n",
    "def encode_gate_info(gate_names, device):\n",
    "    num_gate_types = len(gate_type_map)  # Assicurati che gate_type_map sia definita correttamente\n",
    "    gate_tensor = torch.zeros(1, num_gate_types, dtype=torch.float32, device=device)\n",
    "    \n",
    "    for gate in gate_names:\n",
    "        if gate in gate_type_map:\n",
    "            gate_index = gate_type_map[gate]\n",
    "            gate_tensor[0, gate_index] = 1  # Imposta il valore corretto nel tensore\n",
    "        else:\n",
    "            print(f\"Gate {gate} non trovato nella mappa gate_type_map\")\n",
    "    \n",
    "    return gate_tensor  # [1, num_gate_types]\n",
    "\n",
    "def encode_qubit_info(qubit_info, num_qubits, device):\n",
    "    \"\"\"\n",
    "    Codifica le informazioni sui qubit coinvolti in vettori one-hot.\n",
    "\n",
    "    Args:\n",
    "        qubit_info (list): Lista di qubit o tuple di qubit utilizzati in questo step.\n",
    "        num_qubits (int): Numero totale di qubit nel circuito.\n",
    "        device (torch.device): Dispositivo PyTorch.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensore one-hot di dimensione [1, num_qubits].\n",
    "    \"\"\"\n",
    "    qubit_tensor = torch.zeros(1, num_qubits, dtype=torch.float32, device=device)\n",
    "\n",
    "    for qubit in qubit_info:\n",
    "        if isinstance(qubit, torch.Tensor):\n",
    "            qubit = qubit.item()  # Converte il tensore in un valore scalare\n",
    "\n",
    "        if isinstance(qubit, (tuple, list)):\n",
    "            for q in qubit:\n",
    "                try:\n",
    "                    q = int(q)\n",
    "                    if 0 <= q < num_qubits:\n",
    "                        qubit_tensor[0, q] = 1\n",
    "                    else:\n",
    "                        print(f\"Indice qubit {q} fuori dai limiti (0-{num_qubits-1}).\")\n",
    "                except (ValueError, TypeError) as e:\n",
    "                    print(f\"Errore nel convertire il qubit {q} in int: {e}\")\n",
    "        else:\n",
    "            try:\n",
    "                q = int(qubit)\n",
    "                if 0 <= q < num_qubits:\n",
    "                    qubit_tensor[0, q] = 1\n",
    "                else:\n",
    "                    print(f\"Indice qubit {q} fuori dai limiti (0-{num_qubits-1}).\")\n",
    "            except (ValueError, TypeError) as e:\n",
    "                print(f\"Errore nel convertire il qubit {q} in int: {e}\")\n",
    "\n",
    "    return qubit_tensor  # [1, num_qubits]\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_state(state):\n",
    "    real_number = torch.tensor([[state]], dtype=torch.float32)  # Forma: [1, 1]\n",
    "    return real_number\n",
    "\n",
    "\n",
    "# Definizione della classe DQN\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Definizione della classe ReplayMemory\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done', 'tens_gate_used', 'tens_qubit_used'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# Funzioni di supporto\n",
    "def select_action(state, policy_net, n_actions, device, eps_threshold=0.05):\n",
    "    \"\"\"\n",
    "    Seleziona un'azione utilizzando una politica epsilon-greedy.\n",
    "    \n",
    "    Args:\n",
    "        state (torch.Tensor): Tensore dello stato corrente.\n",
    "        policy_net (nn.Module): Rete neurale policy.\n",
    "        n_actions (int): Numero di azioni disponibili.\n",
    "        device (torch.device): Dispositivo PyTorch.\n",
    "        eps_threshold (float): Soglia epsilon per l'esplorazione.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Tensore dell'azione selezionata.\n",
    "    \"\"\"\n",
    "    sample = random.random()\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "def optimize_model(policy_net, target_net, memory, optimizer, BATCH_SIZE, GAMMA, device):\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    print(\"Ottimizzazione del modello\")\n",
    "\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Creazione della maschera per gli stati non finali\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "\n",
    "    # Estrazione dei next_states non finali\n",
    "    if any(non_final_mask):\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None], dim=0).to(device)  # [N, state_dim]\n",
    "        non_final_tens_gate_used = torch.cat([t for t, d in zip(batch.tens_gate_used, batch.done) if not d], dim=0).to(device)  # [N, num_gate_types]\n",
    "        non_final_tens_qubit_used = torch.cat([q for q, d in zip(batch.tens_qubit_used, batch.done) if not d], dim=0).to(device)  # [N, num_qubits]\n",
    "        \n",
    "        # Verifica delle forme prima della concatenazione\n",
    "        print(f\"non_final_next_states shape: {non_final_next_states.shape}\")  # [N, state_dim]\n",
    "        print(f\"non_final_tens_gate_used shape: {non_final_tens_gate_used.shape}\")  # [N, num_gate_types]\n",
    "        print(f\"non_final_tens_qubit_used shape: {non_final_tens_qubit_used.shape}\")  # [N, num_qubits]\n",
    "\n",
    "        # Concatenazione dei next_state_full\n",
    "        try:\n",
    "            next_state_full = torch.cat((non_final_next_states, non_final_tens_gate_used, non_final_tens_qubit_used), dim=1)  # [N, input_dim]\n",
    "            print(f\"next_state_full shape: {next_state_full.shape}\")  # [N, input_dim]\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Errore durante la concatenazione di next_state_full: {e}\")\n",
    "            return\n",
    "        next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = target_net(next_state_full).max(1)[0]\n",
    "    else:\n",
    "        next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "\n",
    "    # Creazione di state_batch_full\n",
    "    state_batch = torch.cat(batch.state, dim=0).to(device)  # [BATCH_SIZE, state_dim]\n",
    "    tens_gate_used_batch = torch.cat(batch.tens_gate_used, dim=0).to(device)  # [BATCH_SIZE, num_gate_types]\n",
    "    tens_qubit_used_batch = torch.cat(batch.tens_qubit_used, dim=0).to(device)  # [BATCH_SIZE, num_qubits]\n",
    "    state_full_batch = torch.cat((state_batch, tens_gate_used_batch, tens_qubit_used_batch), dim=1)  # [BATCH_SIZE, input_dim]\n",
    "\n",
    "    # Verifica delle forme dopo la concatenazione\n",
    "    print(f\"state_batch shape: {state_batch.shape}\")  # [BATCH_SIZE, state_dim]\n",
    "    print(f\"tens_gate_used_batch shape: {tens_gate_used_batch.shape}\")  # [BATCH_SIZE, num_gate_types]\n",
    "    print(f\"tens_qubit_used_batch shape: {tens_qubit_used_batch.shape}\")  # [BATCH_SIZE, num_qubits]\n",
    "    print(f\"state_full_batch shape: {state_full_batch.shape}\")  # [BATCH_SIZE, input_dim]\n",
    "\n",
    "    # Calcolo dei valori Q(s_t, a)\n",
    "    action_batch = torch.cat(batch.action, dim=0).long().to(device)  # [BATCH_SIZE, 1]\n",
    "    state_action_values = policy_net(state_full_batch).gather(1, action_batch)  # [BATCH_SIZE, 1]\n",
    "\n",
    "    # Calcolo dei valori attesi di Q\n",
    "    reward_batch = torch.cat(batch.reward, dim=0).float().to(device)  # [BATCH_SIZE]\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch  # [BATCH_SIZE]\n",
    "\n",
    "    # Verifica delle forme per Q-values\n",
    "    print(f\"state_action_values shape: {state_action_values.shape}\")  # [BATCH_SIZE, 1]\n",
    "    print(f\"expected_state_action_values shape: {expected_state_action_values.shape}\")  # [BATCH_SIZE]\n",
    "\n",
    "    # Calcolo della perdita usando Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values.squeeze(), expected_state_action_values)\n",
    "\n",
    "    # Ottimizzazione del modello\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Clippa i gradienti per prevenire l'esplosione dei gradienti\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "\n",
    "    # Esegue un passo di ottimizzazione\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Perdita: {loss.item()}\")\n",
    "    print(\"Modello ottimizzato con successo!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if torch.cuda.is_available() or torch.backends.mps.is_available():\\n    num_episodes = 50\\nelse:\\n    num_episodes = 50\\nepisode_results=[]\\nfor i_episode in range(num_episodes):\\n    # Initialize the environment and get its state\\n    print(\"Episiodio numero: \"+ str(i_episode+1)+\"/\"+str(num_episodes))\\n    state, info = env.reset()\\n    #print(info.get(\\'current_accuracy\\'))\\n    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\\n    for t in count():\\n        print(state.shape)\\n        action = select_action(state)\\n        observation, reward, terminated, _ = env.step(action)\\n        print(\"La reward sul test set è :\"+str(reward))\\n        print(\"L\\' episiodio risulta essere: \"+str(terminated))\\n        \\n        #reward = torch.tensor([reward], device=device)\\n        reward = torch.tensor([reward], dtype=torch.float32, device=device)\\n\\n        done = terminated\\n\\n        if terminated:\\n            next_state = None\\n        else:\\n            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\\n        print(\"Lo stato :\"+str(state))\\n        print(\"L\\'action è :\"+str(action))\\n        # Store the transition in memory\\n        memory.push(state, action, next_state, reward)\\n\\n        # Move to the next state\\n        state = next_state\\n\\n        # Perform one step of the optimization (on the policy network)\\n        optimize_model()\\n\\n        # Soft update of the target network\\'s weights\\n        # θ′ ← τ θ + (1 −τ )θ′\\n        target_net_state_dict = target_net.state_dict()\\n        policy_net_state_dict = policy_net.state_dict()\\n        for key in policy_net_state_dict:\\n            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\\n        target_net.load_state_dict(target_net_state_dict)\\n        print(\"Pesi aggiornati\")\\n        if done:\\n            episode_durations.append(t + 1)\\n            #plot_durations()\\n            break\\n        # Store the accuracy and rewards for this episode\\n        episode_results.append({\\n            \"episode\": i_episode,\\n            \"final_reward\": reward.item(),  # Convert to a regular float\\n            \"duration\": t + 1,\\n            \"accuracy\": info.get(\\'current_accuracy\\')\\n        })\\n\\n# Print all results after the loop\\nprint(info.get(\\'circuit\\'))\\nfor result in episode_results:\\n    print(f\"Episode {result[\\'episode\\']}: Reward: {result[\\'final_reward\\']}, Duration: {result[\\'duration\\']}, Accuracy: {result[\\'accuracy\\']}\")\\n\\n\\nprint(\\'Complete\\')\\n#plot_durations(show_result=True)\\n#plt.ioff()\\n#plt.show()\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"if torch.cuda.is_available() or torch.backends.mps.is_available():\n",
    "    num_episodes = 50\n",
    "else:\n",
    "    num_episodes = 50\n",
    "episode_results=[]\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    print(\"Episiodio numero: \"+ str(i_episode+1)+\"/\"+str(num_episodes))\n",
    "    state, info = env.reset()\n",
    "    #print(info.get('current_accuracy'))\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        print(state.shape)\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, _ = env.step(action)\n",
    "        print(\"La reward sul test set è :\"+str(reward))\n",
    "        print(\"L' episiodio risulta essere: \"+str(terminated))\n",
    "        \n",
    "        #reward = torch.tensor([reward], device=device)\n",
    "        reward = torch.tensor([reward], dtype=torch.float32, device=device)\n",
    "\n",
    "        done = terminated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        print(\"Lo stato :\"+str(state))\n",
    "        print(\"L'action è :\"+str(action))\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "        print(\"Pesi aggiornati\")\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            #plot_durations()\n",
    "            break\n",
    "        # Store the accuracy and rewards for this episode\n",
    "        episode_results.append({\n",
    "            \"episode\": i_episode,\n",
    "            \"final_reward\": reward.item(),  # Convert to a regular float\n",
    "            \"duration\": t + 1,\n",
    "            \"accuracy\": info.get('current_accuracy')\n",
    "        })\n",
    "\n",
    "# Print all results after the loop\n",
    "print(info.get('circuit'))\n",
    "for result in episode_results:\n",
    "    print(f\"Episode {result['episode']}: Reward: {result['final_reward']}, Duration: {result['duration']}, Accuracy: {result['accuracy']}\")\n",
    "\n",
    "\n",
    "print('Complete')\n",
    "#plot_durations(show_result=True)\n",
    "#plt.ioff()\n",
    "#plt.show()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episodio numero: 1/15\n",
      "Forma di tens_gate_used: torch.Size([1, 11])\n",
      "Forma di tens_qubit_used: torch.Size([1, 4])\n",
      "\n",
      "Forma dello stato: torch.Size([1, 1])\n",
      "Forma dello stato concatenato: torch.Size([1, 16])\n",
      "L'azione selezionata: tensor([[15]], device='cuda:0')\n",
      "Current Quantum Circuit:\n",
      "     \n",
      "q_0: \n",
      "     \n",
      "q_1: \n",
      "     \n",
      "q_2: \n",
      "     \n",
      "q_3: \n",
      "     \n",
      "[0 1 0 0 0 0 2 1 1 2 1 1 2 1 0 2 1 0 2 2 1 2 2 2 1 2 0 1 0 1 0 1 2 1 1 2 1\n",
      " 1 1 0 2 0 0 0 0 1 1 0 2 0 1 1 2 0]\n",
      "Quantum Circuit:\n",
      "          \n",
      "q_0: ─────\n",
      "          \n",
      "q_1: ─────\n",
      "          \n",
      "q_2: ─────\n",
      "     ┌───┐\n",
      "q_3: ┤ H ├\n",
      "     └───┘\n",
      "FEATURE MAP:\n",
      "        ┌──────────────────┐┌────────────────────────────────────┐\n",
      "q310_0: ┤0                 ├┤0                                   ├\n",
      "        │                  ││                                    │\n",
      "q310_1: ┤1                 ├┤1                                   ├\n",
      "        │  circuit-4396507 ││  ZZFeatureMap(x[0],x[1],x[2],x[3]) │\n",
      "q310_2: ┤2                 ├┤2                                   ├\n",
      "        │                  ││                                    │\n",
      "q310_3: ┤3                 ├┤3                                   ├\n",
      "        └──────────────────┘└────────────────────────────────────┘\n",
      "Cross-validation scores: [0.68 0.68 0.68 0.8  0.75]\n",
      "Average score: 0.718\n",
      "QSVC classification train score: 0.9516129032258065\n",
      "QSVC classification test score: 0.7222222222222222\n",
      "Prima itrazione:0.9516129032258065\n",
      "Reward: 0.9999987357695994\n",
      "La reward sul test set è: 0.9999987357695994\n",
      "L'episodio risulta essere: False\n",
      "Il tensore dei qubit:\n",
      "tensor([[0., 0., 0., 1.]], device='cuda:0')\n",
      "Il tensore dei gate:\n",
      "tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n",
      "Lo stato corrente: tensor([[1.]], device='cuda:0')\n",
      "L'azione eseguita: tensor([[15]], device='cuda:0')\n",
      "Pesi aggiornati\n",
      "\n",
      "Forma dello stato: torch.Size([1, 1])\n",
      "Forma dello stato concatenato: torch.Size([1, 16])\n",
      "L'azione selezionata: tensor([[3]], device='cuda:0')\n",
      "Current Quantum Circuit:\n",
      "          \n",
      "q_0: ─────\n",
      "          \n",
      "q_1: ─────\n",
      "          \n",
      "q_2: ─────\n",
      "     ┌───┐\n",
      "q_3: ┤ H ├\n",
      "     └───┘\n",
      "[0 1 0 0 0 0 2 1 1 2 1 1 2 1 0 2 1 0 2 2 1 2 2 2 1 2 0 1 0 1 0 1 2 1 1 2 1\n",
      " 1 1 0 2 0 0 0 0 1 1 0 2 0 1 1 2 0]\n",
      "Quantum Circuit:\n",
      "     ┌───┐\n",
      "q_0: ┤ X ├\n",
      "     └─┬─┘\n",
      "q_1: ──■──\n",
      "          \n",
      "q_2: ─────\n",
      "     ┌───┐\n",
      "q_3: ┤ H ├\n",
      "     └───┘\n",
      "FEATURE MAP:\n",
      "        ┌──────────────────┐┌────────────────────────────────────┐\n",
      "q310_0: ┤0                 ├┤0                                   ├\n",
      "        │                  ││                                    │\n",
      "q310_1: ┤1                 ├┤1                                   ├\n",
      "        │  circuit-4396507 ││  ZZFeatureMap(x[0],x[1],x[2],x[3]) │\n",
      "q310_2: ┤2                 ├┤2                                   ├\n",
      "        │                  ││                                    │\n",
      "q310_3: ┤3                 ├┤3                                   ├\n",
      "        └──────────────────┘└────────────────────────────────────┘\n",
      "Cross-validation scores: [0.68 0.68 0.68 0.8  0.75]\n",
      "Average score: 0.718\n",
      "QSVC classification train score: 0.9516129032258065\n",
      "QSVC classification test score: 0.7222222222222222\n",
      "Reward: 0.0\n",
      "La reward sul test set è: 0.0\n",
      "L'episodio risulta essere: False\n",
      "Il tensore dei qubit:\n",
      "tensor([[1., 1., 0., 0.]], device='cuda:0')\n",
      "Il tensore dei gate:\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n",
      "Lo stato corrente: tensor([[0.0484]], device='cuda:0')\n",
      "L'azione eseguita: tensor([[3]], device='cuda:0')\n",
      "Pesi aggiornati\n",
      "\n",
      "Forma dello stato: torch.Size([1, 1])\n",
      "Forma dello stato concatenato: torch.Size([1, 16])\n",
      "L'azione selezionata: tensor([[21]], device='cuda:0')\n",
      "Current Quantum Circuit:\n",
      "     ┌───┐\n",
      "q_0: ┤ X ├\n",
      "     └─┬─┘\n",
      "q_1: ──■──\n",
      "          \n",
      "q_2: ─────\n",
      "     ┌───┐\n",
      "q_3: ┤ H ├\n",
      "     └───┘\n",
      "[0 1 0 0 0 0 2 1 1 2 1 1 2 1 0 2 1 0 2 2 1 2 2 2 1 2 0 1 0 1 0 1 2 1 1 2 1\n",
      " 1 1 0 2 0 0 0 0 1 1 0 2 0 1 1 2 0]\n",
      "Quantum Circuit:\n",
      "     ┌───┐               \n",
      "q_0: ┤ X ├───────────────\n",
      "     └─┬─┘┌─────────────┐\n",
      "q_1: ──■──┤ Ry(0.70581) ├\n",
      "          └─────────────┘\n",
      "q_2: ────────────────────\n",
      "     ┌───┐               \n",
      "q_3: ┤ H ├───────────────\n",
      "     └───┘               \n",
      "FEATURE MAP:\n",
      "        ┌──────────────────┐┌────────────────────────────────────┐\n",
      "q310_0: ┤0                 ├┤0                                   ├\n",
      "        │                  ││                                    │\n",
      "q310_1: ┤1                 ├┤1                                   ├\n",
      "        │  circuit-4396507 ││  ZZFeatureMap(x[0],x[1],x[2],x[3]) │\n",
      "q310_2: ┤2                 ├┤2                                   ├\n",
      "        │                  ││                                    │\n",
      "q310_3: ┤3                 ├┤3                                   ├\n",
      "        └──────────────────┘└────────────────────────────────────┘\n",
      "Cross-validation scores: [0.76 0.64 0.72 0.84 0.75]\n",
      "Average score: 0.742\n",
      "QSVC classification train score: 0.9516129032258065\n",
      "QSVC classification test score: 0.7037037037037037\n",
      "Reward: 0.0\n",
      "La reward sul test set è: 0.0\n",
      "L'episodio risulta essere: False\n",
      "Il tensore dei qubit:\n",
      "tensor([[0., 1., 0., 0.]], device='cuda:0')\n",
      "Il tensore dei gate:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]], device='cuda:0')\n",
      "Lo stato corrente: tensor([[0.0484]], device='cuda:0')\n",
      "L'azione eseguita: tensor([[21]], device='cuda:0')\n",
      "Pesi aggiornati\n",
      "\n",
      "Forma dello stato: torch.Size([1, 1])\n",
      "Forma dello stato concatenato: torch.Size([1, 16])\n",
      "L'azione selezionata: tensor([[3]], device='cuda:0')\n",
      "Current Quantum Circuit:\n",
      "     ┌───┐               \n",
      "q_0: ┤ X ├───────────────\n",
      "     └─┬─┘┌─────────────┐\n",
      "q_1: ──■──┤ Ry(0.70581) ├\n",
      "          └─────────────┘\n",
      "q_2: ────────────────────\n",
      "     ┌───┐               \n",
      "q_3: ┤ H ├───────────────\n",
      "     └───┘               \n",
      "[0 1 0 0 0 0 2 1 1 2 1 1 2 1 0 2 1 0 2 2 1 2 2 2 1 2 0 1 0 1 0 1 2 1 1 2 1\n",
      " 1 1 0 2 0 0 0 0 1 1 0 2 0 1 1 2 0]\n",
      "Quantum Circuit:\n",
      "     ┌───┐               ┌───┐\n",
      "q_0: ┤ X ├───────────────┤ X ├\n",
      "     └─┬─┘┌─────────────┐└─┬─┘\n",
      "q_1: ──■──┤ Ry(0.70581) ├──■──\n",
      "          └─────────────┘     \n",
      "q_2: ─────────────────────────\n",
      "     ┌───┐                    \n",
      "q_3: ┤ H ├────────────────────\n",
      "     └───┘                    \n",
      "FEATURE MAP:\n",
      "        ┌──────────────────┐┌────────────────────────────────────┐\n",
      "q310_0: ┤0                 ├┤0                                   ├\n",
      "        │                  ││                                    │\n",
      "q310_1: ┤1                 ├┤1                                   ├\n",
      "        │  circuit-4396507 ││  ZZFeatureMap(x[0],x[1],x[2],x[3]) │\n",
      "q310_2: ┤2                 ├┤2                                   ├\n",
      "        │                  ││                                    │\n",
      "q310_3: ┤3                 ├┤3                                   ├\n",
      "        └──────────────────┘└────────────────────────────────────┘\n",
      "Cross-validation scores: [0.8        0.68       0.68       0.84       0.79166667]\n",
      "Average score: 0.7583333333333333\n",
      "QSVC classification train score: 0.9516129032258065\n",
      "QSVC classification test score: 0.6851851851851852\n",
      "Reward: 0.0\n",
      "La reward sul test set è: 0.0\n",
      "L'episodio risulta essere: False\n",
      "Il tensore dei qubit:\n",
      "tensor([[1., 1., 0., 0.]], device='cuda:0')\n",
      "Il tensore dei gate:\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n",
      "Lo stato corrente: tensor([[0.0484]], device='cuda:0')\n",
      "L'azione eseguita: tensor([[3]], device='cuda:0')\n",
      "Pesi aggiornati\n",
      "\n",
      "Forma dello stato: torch.Size([1, 1])\n",
      "Forma dello stato concatenato: torch.Size([1, 16])\n",
      "L'azione selezionata: tensor([[5]], device='cuda:0')\n",
      "Current Quantum Circuit:\n",
      "     ┌───┐               ┌───┐\n",
      "q_0: ┤ X ├───────────────┤ X ├\n",
      "     └─┬─┘┌─────────────┐└─┬─┘\n",
      "q_1: ──■──┤ Ry(0.70581) ├──■──\n",
      "          └─────────────┘     \n",
      "q_2: ─────────────────────────\n",
      "     ┌───┐                    \n",
      "q_3: ┤ H ├────────────────────\n",
      "     └───┘                    \n",
      "[0 1 0 0 0 0 2 1 1 2 1 1 2 1 0 2 1 0 2 2 1 2 2 2 1 2 0 1 0 1 0 1 2 1 1 2 1\n",
      " 1 1 0 2 0 0 0 0 1 1 0 2 0 1 1 2 0]\n",
      "Quantum Circuit:\n",
      "     ┌───┐               ┌───┐     \n",
      "q_0: ┤ X ├───────────────┤ X ├─────\n",
      "     └─┬─┘┌─────────────┐└─┬─┘     \n",
      "q_1: ──■──┤ Ry(0.70581) ├──■────■──\n",
      "          └─────────────┘       │  \n",
      "q_2: ───────────────────────────┼──\n",
      "     ┌───┐                    ┌─┴─┐\n",
      "q_3: ┤ H ├────────────────────┤ X ├\n",
      "     └───┘                    └───┘\n",
      "FEATURE MAP:\n",
      "        ┌──────────────────┐┌────────────────────────────────────┐\n",
      "q310_0: ┤0                 ├┤0                                   ├\n",
      "        │                  ││                                    │\n",
      "q310_1: ┤1                 ├┤1                                   ├\n",
      "        │  circuit-4396507 ││  ZZFeatureMap(x[0],x[1],x[2],x[3]) │\n",
      "q310_2: ┤2                 ├┤2                                   ├\n",
      "        │                  ││                                    │\n",
      "q310_3: ┤3                 ├┤3                                   ├\n",
      "        └──────────────────┘└────────────────────────────────────┘\n",
      "Cross-validation scores: [0.8        0.68       0.68       0.84       0.79166667]\n",
      "Average score: 0.7583333333333333\n",
      "QSVC classification train score: 0.9516129032258065\n",
      "QSVC classification test score: 0.6851851851851852\n",
      "Reward: 0.0\n",
      "Episodio terminato: limite di passi raggiunto.\n",
      "La reward sul test set è: 0.0\n",
      "L'episodio risulta essere: True\n",
      "Il tensore dei qubit:\n",
      "tensor([[0., 1., 0., 1.]], device='cuda:0')\n",
      "Il tensore dei gate:\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n",
      "Lo stato corrente: tensor([[0.0484]], device='cuda:0')\n",
      "L'azione eseguita: tensor([[5]], device='cuda:0')\n",
      "Pesi aggiornati\n",
      "\n",
      "Episodio numero: 2/15\n",
      "Forma di tens_gate_used: torch.Size([1, 11])\n",
      "Forma di tens_qubit_used: torch.Size([1, 4])\n",
      "\n",
      "Forma dello stato: torch.Size([1, 1])\n",
      "Forma dello stato concatenato: torch.Size([1, 16])\n",
      "L'azione selezionata: tensor([[9]], device='cuda:0')\n",
      "Current Quantum Circuit:\n",
      "     \n",
      "q_0: \n",
      "     \n",
      "q_1: \n",
      "     \n",
      "q_2: \n",
      "     \n",
      "q_3: \n",
      "     \n",
      "[0 1 0 0 0 0 2 1 1 2 1 1 2 1 0 2 1 0 2 2 1 2 2 2 1 2 0 1 0 1 0 1 2 1 1 2 1\n",
      " 1 1 0 2 0 0 0 0 1 1 0 2 0 1 1 2 0]\n",
      "Quantum Circuit:\n",
      "     ┌───┐\n",
      "q_0: ┤ X ├\n",
      "     └─┬─┘\n",
      "q_1: ──┼──\n",
      "       │  \n",
      "q_2: ──┼──\n",
      "       │  \n",
      "q_3: ──■──\n",
      "          \n",
      "FEATURE MAP:\n",
      "        ┌──────────────────┐┌────────────────────────────────────┐\n",
      "q310_0: ┤0                 ├┤0                                   ├\n",
      "        │                  ││                                    │\n",
      "q310_1: ┤1                 ├┤1                                   ├\n",
      "        │  circuit-4689589 ││  ZZFeatureMap(x[0],x[1],x[2],x[3]) │\n",
      "q310_2: ┤2                 ├┤2                                   ├\n",
      "        │                  ││                                    │\n",
      "q310_3: ┤3                 ├┤3                                   ├\n",
      "        └──────────────────┘└────────────────────────────────────┘\n",
      "Cross-validation scores: [0.8  0.8  0.72 0.76 0.75]\n",
      "Average score: 0.766\n",
      "QSVC classification train score: 0.9032258064516129\n",
      "QSVC classification test score: 0.7407407407407407\n",
      "Prima itrazione:0.9032258064516129\n",
      "Reward: 0.999997387600009\n",
      "La reward sul test set è: 0.999997387600009\n",
      "L'episodio risulta essere: False\n",
      "Il tensore dei qubit:\n",
      "tensor([[1., 0., 0., 1.]], device='cuda:0')\n",
      "Il tensore dei gate:\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n",
      "Lo stato corrente: tensor([[0.0484]], device='cuda:0')\n",
      "L'azione eseguita: tensor([[9]], device='cuda:0')\n",
      "Pesi aggiornati\n",
      "\n",
      "Forma dello stato: torch.Size([1, 1])\n",
      "Forma dello stato concatenato: torch.Size([1, 16])\n",
      "L'azione selezionata: tensor([[14]], device='cuda:0')\n",
      "Current Quantum Circuit:\n",
      "     ┌───┐\n",
      "q_0: ┤ X ├\n",
      "     └─┬─┘\n",
      "q_1: ──┼──\n",
      "       │  \n",
      "q_2: ──┼──\n",
      "       │  \n",
      "q_3: ──■──\n",
      "          \n",
      "[0 1 0 0 0 0 2 1 1 2 1 1 2 1 0 2 1 0 2 2 1 2 2 2 1 2 0 1 0 1 0 1 2 1 1 2 1\n",
      " 1 1 0 2 0 0 0 0 1 1 0 2 0 1 1 2 0]\n",
      "Quantum Circuit:\n",
      "     ┌───┐     \n",
      "q_0: ┤ X ├─────\n",
      "     └─┬─┘     \n",
      "q_1: ──┼───────\n",
      "       │  ┌───┐\n",
      "q_2: ──┼──┤ H ├\n",
      "       │  └───┘\n",
      "q_3: ──■───────\n",
      "               \n",
      "FEATURE MAP:\n",
      "        ┌──────────────────┐┌────────────────────────────────────┐\n",
      "q310_0: ┤0                 ├┤0                                   ├\n",
      "        │                  ││                                    │\n",
      "q310_1: ┤1                 ├┤1                                   ├\n",
      "        │  circuit-4689589 ││  ZZFeatureMap(x[0],x[1],x[2],x[3]) │\n",
      "q310_2: ┤2                 ├┤2                                   ├\n",
      "        │                  ││                                    │\n",
      "q310_3: ┤3                 ├┤3                                   ├\n",
      "        └──────────────────┘└────────────────────────────────────┘\n",
      "Cross-validation scores: [0.56       0.6        0.64       0.72       0.70833333]\n",
      "Average score: 0.6456666666666668\n",
      "QSVC classification train score: 0.9193548387096774\n",
      "QSVC classification test score: 0.7037037037037037\n",
      "Reward: 0.12038112714680871\n",
      "La reward sul test set è: 0.12038112714680871\n",
      "L'episodio risulta essere: False\n",
      "Il tensore dei qubit:\n",
      "tensor([[0., 0., 1., 0.]], device='cuda:0')\n",
      "Il tensore dei gate:\n",
      "tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n",
      "Lo stato corrente: tensor([[0.0968]], device='cuda:0')\n",
      "L'azione eseguita: tensor([[14]], device='cuda:0')\n",
      "Pesi aggiornati\n",
      "\n",
      "Forma dello stato: torch.Size([1, 1])\n",
      "Forma dello stato concatenato: torch.Size([1, 16])\n",
      "L'azione selezionata: tensor([[2]], device='cuda:0')\n",
      "Current Quantum Circuit:\n",
      "     ┌───┐     \n",
      "q_0: ┤ X ├─────\n",
      "     └─┬─┘     \n",
      "q_1: ──┼───────\n",
      "       │  ┌───┐\n",
      "q_2: ──┼──┤ H ├\n",
      "       │  └───┘\n",
      "q_3: ──■───────\n",
      "               \n",
      "[0 1 0 0 0 0 2 1 1 2 1 1 2 1 0 2 1 0 2 2 1 2 2 2 1 2 0 1 0 1 0 1 2 1 1 2 1\n",
      " 1 1 0 2 0 0 0 0 1 1 0 2 0 1 1 2 0]\n",
      "Quantum Circuit:\n",
      "     ┌───┐          \n",
      "q_0: ┤ X ├───────■──\n",
      "     └─┬─┘       │  \n",
      "q_1: ──┼─────────┼──\n",
      "       │  ┌───┐  │  \n",
      "q_2: ──┼──┤ H ├──┼──\n",
      "       │  └───┘┌─┴─┐\n",
      "q_3: ──■───────┤ X ├\n",
      "               └───┘\n",
      "FEATURE MAP:\n",
      "        ┌──────────────────┐┌────────────────────────────────────┐\n",
      "q310_0: ┤0                 ├┤0                                   ├\n",
      "        │                  ││                                    │\n",
      "q310_1: ┤1                 ├┤1                                   ├\n",
      "        │  circuit-4689589 ││  ZZFeatureMap(x[0],x[1],x[2],x[3]) │\n",
      "q310_2: ┤2                 ├┤2                                   ├\n",
      "        │                  ││                                    │\n",
      "q310_3: ┤3                 ├┤3                                   ├\n",
      "        └──────────────────┘└────────────────────────────────────┘\n",
      "Cross-validation scores: [0.56       0.6        0.64       0.72       0.70833333]\n",
      "Average score: 0.6456666666666668\n",
      "QSVC classification train score: 0.9193548387096774\n",
      "QSVC classification test score: 0.7037037037037037\n",
      "Reward: 0.0\n",
      "La reward sul test set è: 0.0\n",
      "L'episodio risulta essere: False\n",
      "Il tensore dei qubit:\n",
      "tensor([[1., 0., 0., 1.]], device='cuda:0')\n",
      "Il tensore dei gate:\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n",
      "Lo stato corrente: tensor([[0.0806]], device='cuda:0')\n",
      "L'azione eseguita: tensor([[2]], device='cuda:0')\n",
      "Pesi aggiornati\n",
      "\n",
      "Forma dello stato: torch.Size([1, 1])\n",
      "Forma dello stato concatenato: torch.Size([1, 16])\n",
      "L'azione selezionata: tensor([[20]], device='cuda:0')\n",
      "Current Quantum Circuit:\n",
      "     ┌───┐          \n",
      "q_0: ┤ X ├───────■──\n",
      "     └─┬─┘       │  \n",
      "q_1: ──┼─────────┼──\n",
      "       │  ┌───┐  │  \n",
      "q_2: ──┼──┤ H ├──┼──\n",
      "       │  └───┘┌─┴─┐\n",
      "q_3: ──■───────┤ X ├\n",
      "               └───┘\n",
      "[0 1 0 0 0 0 2 1 1 2 1 1 2 1 0 2 1 0 2 2 1 2 2 2 1 2 0 1 0 1 0 1 2 1 1 2 1\n",
      " 1 1 0 2 0 0 0 0 1 1 0 2 0 1 1 2 0]\n",
      "Quantum Circuit:\n",
      "     ┌───┐          ┌────────────┐\n",
      "q_0: ┤ X ├───────■──┤ Ry(5.2627) ├\n",
      "     └─┬─┘       │  └────────────┘\n",
      "q_1: ──┼─────────┼────────────────\n",
      "       │  ┌───┐  │                \n",
      "q_2: ──┼──┤ H ├──┼────────────────\n",
      "       │  └───┘┌─┴─┐              \n",
      "q_3: ──■───────┤ X ├──────────────\n",
      "               └───┘              \n",
      "FEATURE MAP:\n",
      "        ┌──────────────────┐┌────────────────────────────────────┐\n",
      "q310_0: ┤0                 ├┤0                                   ├\n",
      "        │                  ││                                    │\n",
      "q310_1: ┤1                 ├┤1                                   ├\n",
      "        │  circuit-4689589 ││  ZZFeatureMap(x[0],x[1],x[2],x[3]) │\n",
      "q310_2: ┤2                 ├┤2                                   ├\n",
      "        │                  ││                                    │\n",
      "q310_3: ┤3                 ├┤3                                   ├\n",
      "        └──────────────────┘└────────────────────────────────────┘\n",
      "Cross-validation scores: [0.64 0.72 0.68 0.72 0.75]\n",
      "Average score: 0.702\n",
      "QSVC classification train score: 0.9354838709677419\n",
      "QSVC classification test score: 0.7407407407407407\n",
      "Reward: 0.12038112714680871\n",
      "La reward sul test set è: 0.12038112714680871\n",
      "L'episodio risulta essere: False\n",
      "Il tensore dei qubit:\n",
      "tensor([[1., 0., 0., 0.]], device='cuda:0')\n",
      "Il tensore dei gate:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]], device='cuda:0')\n",
      "Lo stato corrente: tensor([[0.0806]], device='cuda:0')\n",
      "L'azione eseguita: tensor([[20]], device='cuda:0')\n",
      "Pesi aggiornati\n",
      "\n",
      "Forma dello stato: torch.Size([1, 1])\n",
      "Forma dello stato concatenato: torch.Size([1, 16])\n",
      "L'azione selezionata: tensor([[5]], device='cuda:0')\n",
      "Current Quantum Circuit:\n",
      "     ┌───┐          ┌────────────┐\n",
      "q_0: ┤ X ├───────■──┤ Ry(5.2627) ├\n",
      "     └─┬─┘       │  └────────────┘\n",
      "q_1: ──┼─────────┼────────────────\n",
      "       │  ┌───┐  │                \n",
      "q_2: ──┼──┤ H ├──┼────────────────\n",
      "       │  └───┘┌─┴─┐              \n",
      "q_3: ──■───────┤ X ├──────────────\n",
      "               └───┘              \n",
      "[0 1 0 0 0 0 2 1 1 2 1 1 2 1 0 2 1 0 2 2 1 2 2 2 1 2 0 1 0 1 0 1 2 1 1 2 1\n",
      " 1 1 0 2 0 0 0 0 1 1 0 2 0 1 1 2 0]\n",
      "Quantum Circuit:\n",
      "     ┌───┐          ┌────────────┐\n",
      "q_0: ┤ X ├───────■──┤ Ry(5.2627) ├\n",
      "     └─┬─┘       │  └────────────┘\n",
      "q_1: ──┼─────────┼────────■───────\n",
      "       │  ┌───┐  │        │       \n",
      "q_2: ──┼──┤ H ├──┼────────┼───────\n",
      "       │  └───┘┌─┴─┐    ┌─┴─┐     \n",
      "q_3: ──■───────┤ X ├────┤ X ├─────\n",
      "               └───┘    └───┘     \n",
      "FEATURE MAP:\n",
      "        ┌──────────────────┐┌────────────────────────────────────┐\n",
      "q310_0: ┤0                 ├┤0                                   ├\n",
      "        │                  ││                                    │\n",
      "q310_1: ┤1                 ├┤1                                   ├\n",
      "        │  circuit-4689589 ││  ZZFeatureMap(x[0],x[1],x[2],x[3]) │\n",
      "q310_2: ┤2                 ├┤2                                   ├\n",
      "        │                  ││                                    │\n",
      "q310_3: ┤3                 ├┤3                                   ├\n",
      "        └──────────────────┘└────────────────────────────────────┘\n",
      "Cross-validation scores: [0.64 0.72 0.68 0.72 0.75]\n",
      "Average score: 0.702\n",
      "QSVC classification train score: 0.9354838709677419\n",
      "QSVC classification test score: 0.7407407407407407\n",
      "Reward: 0.0\n",
      "Episodio terminato: limite di passi raggiunto.\n",
      "La reward sul test set è: 0.0\n",
      "L'episodio risulta essere: True\n",
      "Il tensore dei qubit:\n",
      "tensor([[0., 1., 0., 1.]], device='cuda:0')\n",
      "Il tensore dei gate:\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n",
      "Lo stato corrente: tensor([[0.0645]], device='cuda:0')\n",
      "L'azione eseguita: tensor([[5]], device='cuda:0')\n",
      "Ottimizzazione del modello\n",
      "non_final_next_states shape: torch.Size([8, 1])\n",
      "non_final_tens_gate_used shape: torch.Size([8, 11])\n",
      "non_final_tens_qubit_used shape: torch.Size([8, 4])\n",
      "next_state_full shape: torch.Size([8, 16])\n",
      "state_batch shape: torch.Size([10, 1])\n",
      "tens_gate_used_batch shape: torch.Size([10, 11])\n",
      "tens_qubit_used_batch shape: torch.Size([10, 4])\n",
      "state_full_batch shape: torch.Size([10, 16])\n",
      "state_action_values shape: torch.Size([10, 1])\n",
      "expected_state_action_values shape: torch.Size([10])\n",
      "Perdita: 0.14150729775428772\n",
      "Modello ottimizzato con successo!\n",
      "Pesi aggiornati\n",
      "\n",
      "Episodio numero: 3/15\n",
      "Forma di tens_gate_used: torch.Size([1, 11])\n",
      "Forma di tens_qubit_used: torch.Size([1, 4])\n",
      "\n",
      "Forma dello stato: torch.Size([1, 1])\n",
      "Forma dello stato concatenato: torch.Size([1, 16])\n",
      "L'azione selezionata: tensor([[15]], device='cuda:0')\n",
      "Current Quantum Circuit:\n",
      "     \n",
      "q_0: \n",
      "     \n",
      "q_1: \n",
      "     \n",
      "q_2: \n",
      "     \n",
      "q_3: \n",
      "     \n",
      "[0 1 0 0 0 0 2 1 1 2 1 1 2 1 0 2 1 0 2 2 1 2 2 2 1 2 0 1 0 1 0 1 2 1 1 2 1\n",
      " 1 1 0 2 0 0 0 0 1 1 0 2 0 1 1 2 0]\n",
      "Quantum Circuit:\n",
      "          \n",
      "q_0: ─────\n",
      "          \n",
      "q_1: ─────\n",
      "          \n",
      "q_2: ─────\n",
      "     ┌───┐\n",
      "q_3: ┤ H ├\n",
      "     └───┘\n",
      "FEATURE MAP:\n",
      "        ┌──────────────────┐┌────────────────────────────────────┐\n",
      "q310_0: ┤0                 ├┤0                                   ├\n",
      "        │                  ││                                    │\n",
      "q310_1: ┤1                 ├┤1                                   ├\n",
      "        │  circuit-4982669 ││  ZZFeatureMap(x[0],x[1],x[2],x[3]) │\n",
      "q310_2: ┤2                 ├┤2                                   ├\n",
      "        │                  ││                                    │\n",
      "q310_3: ┤3                 ├┤3                                   ├\n",
      "        └──────────────────┘└────────────────────────────────────┘\n",
      "Cross-validation scores: [0.68 0.68 0.68 0.8  0.75]\n",
      "Average score: 0.718\n",
      "QSVC classification train score: 0.9516129032258065\n",
      "QSVC classification test score: 0.7222222222222222\n",
      "Prima itrazione:0.9516129032258065\n",
      "Reward: 0.9999987357695994\n",
      "La reward sul test set è: 0.9999987357695994\n",
      "L'episodio risulta essere: False\n",
      "Il tensore dei qubit:\n",
      "tensor([[0., 0., 0., 1.]], device='cuda:0')\n",
      "Il tensore dei gate:\n",
      "tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n",
      "Lo stato corrente: tensor([[0.0645]], device='cuda:0')\n",
      "L'azione eseguita: tensor([[15]], device='cuda:0')\n",
      "Ottimizzazione del modello\n",
      "non_final_next_states shape: torch.Size([8, 1])\n",
      "non_final_tens_gate_used shape: torch.Size([8, 11])\n",
      "non_final_tens_qubit_used shape: torch.Size([8, 4])\n",
      "next_state_full shape: torch.Size([8, 16])\n",
      "state_batch shape: torch.Size([10, 1])\n",
      "tens_gate_used_batch shape: torch.Size([10, 11])\n",
      "tens_qubit_used_batch shape: torch.Size([10, 4])\n",
      "state_full_batch shape: torch.Size([10, 16])\n",
      "state_action_values shape: torch.Size([10, 1])\n",
      "expected_state_action_values shape: torch.Size([10])\n",
      "Perdita: 0.19476936757564545\n",
      "Modello ottimizzato con successo!\n",
      "Pesi aggiornati\n",
      "\n",
      "Forma dello stato: torch.Size([1, 1])\n",
      "Forma dello stato concatenato: torch.Size([1, 16])\n",
      "L'azione selezionata: tensor([[11]], device='cuda:0')\n",
      "Current Quantum Circuit:\n",
      "          \n",
      "q_0: ─────\n",
      "          \n",
      "q_1: ─────\n",
      "          \n",
      "q_2: ─────\n",
      "     ┌───┐\n",
      "q_3: ┤ H ├\n",
      "     └───┘\n",
      "[0 1 0 0 0 0 2 1 1 2 1 1 2 1 0 2 1 0 2 2 1 2 2 2 1 2 0 1 0 1 0 1 2 1 1 2 1\n",
      " 1 1 0 2 0 0 0 0 1 1 0 2 0 1 1 2 0]\n",
      "Quantum Circuit:\n",
      "               \n",
      "q_0: ──────────\n",
      "               \n",
      "q_1: ──────────\n",
      "          ┌───┐\n",
      "q_2: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_3: ┤ H ├──■──\n",
      "     └───┘     \n",
      "FEATURE MAP:\n",
      "        ┌──────────────────┐┌────────────────────────────────────┐\n",
      "q310_0: ┤0                 ├┤0                                   ├\n",
      "        │                  ││                                    │\n",
      "q310_1: ┤1                 ├┤1                                   ├\n",
      "        │  circuit-4982669 ││  ZZFeatureMap(x[0],x[1],x[2],x[3]) │\n",
      "q310_2: ┤2                 ├┤2                                   ├\n",
      "        │                  ││                                    │\n",
      "q310_3: ┤3                 ├┤3                                   ├\n",
      "        └──────────────────┘└────────────────────────────────────┘\n",
      "Cross-validation scores: [0.76       0.72       0.72       0.84       0.79166667]\n",
      "Average score: 0.7663333333333333\n",
      "QSVC classification train score: 0.9354838709677419\n",
      "QSVC classification test score: 0.7777777777777778\n",
      "Reward: -0.12038112714680949\n",
      "La reward sul test set è: -0.12038112714680949\n",
      "L'episodio risulta essere: False\n",
      "Il tensore dei qubit:\n",
      "tensor([[0., 0., 1., 1.]], device='cuda:0')\n",
      "Il tensore dei gate:\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n",
      "Lo stato corrente: tensor([[0.0484]], device='cuda:0')\n",
      "L'azione eseguita: tensor([[11]], device='cuda:0')\n",
      "Ottimizzazione del modello\n",
      "non_final_next_states shape: torch.Size([8, 1])\n",
      "non_final_tens_gate_used shape: torch.Size([8, 11])\n",
      "non_final_tens_qubit_used shape: torch.Size([8, 4])\n",
      "next_state_full shape: torch.Size([8, 16])\n",
      "state_batch shape: torch.Size([10, 1])\n",
      "tens_gate_used_batch shape: torch.Size([10, 11])\n",
      "tens_qubit_used_batch shape: torch.Size([10, 4])\n",
      "state_full_batch shape: torch.Size([10, 16])\n",
      "state_action_values shape: torch.Size([10, 1])\n",
      "expected_state_action_values shape: torch.Size([10])\n",
      "Perdita: 0.17910419404506683\n",
      "Modello ottimizzato con successo!\n",
      "Pesi aggiornati\n",
      "\n",
      "Forma dello stato: torch.Size([1, 1])\n",
      "Forma dello stato concatenato: torch.Size([1, 16])\n",
      "L'azione selezionata: tensor([[20]], device='cuda:0')\n",
      "Current Quantum Circuit:\n",
      "               \n",
      "q_0: ──────────\n",
      "               \n",
      "q_1: ──────────\n",
      "          ┌───┐\n",
      "q_2: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_3: ┤ H ├──■──\n",
      "     └───┘     \n",
      "[0 1 0 0 0 0 2 1 1 2 1 1 2 1 0 2 1 0 2 2 1 2 2 2 1 2 0 1 0 1 0 1 2 1 1 2 1\n",
      " 1 1 0 2 0 0 0 0 1 1 0 2 0 1 1 2 0]\n",
      "Quantum Circuit:\n",
      "     ┌─────────────┐     \n",
      "q_0: ┤ Ry(0.82862) ├─────\n",
      "     └─────────────┘     \n",
      "q_1: ────────────────────\n",
      "                    ┌───┐\n",
      "q_2: ───────────────┤ X ├\n",
      "          ┌───┐     └─┬─┘\n",
      "q_3: ─────┤ H ├───────■──\n",
      "          └───┘          \n",
      "FEATURE MAP:\n",
      "        ┌──────────────────┐┌────────────────────────────────────┐\n",
      "q310_0: ┤0                 ├┤0                                   ├\n",
      "        │                  ││                                    │\n",
      "q310_1: ┤1                 ├┤1                                   ├\n",
      "        │  circuit-4982669 ││  ZZFeatureMap(x[0],x[1],x[2],x[3]) │\n",
      "q310_2: ┤2                 ├┤2                                   ├\n",
      "        │                  ││                                    │\n",
      "q310_3: ┤3                 ├┤3                                   ├\n",
      "        └──────────────────┘└────────────────────────────────────┘\n",
      "Cross-validation scores: [0.76       0.88       0.68       0.96       0.79166667]\n",
      "Average score: 0.8143333333333335\n",
      "QSVC classification train score: 0.9354838709677419\n",
      "QSVC classification test score: 0.7962962962962963\n",
      "Reward: 0.0\n",
      "La reward sul test set è: 0.0\n",
      "L'episodio risulta essere: False\n",
      "Il tensore dei qubit:\n",
      "tensor([[1., 0., 0., 0.]], device='cuda:0')\n",
      "Il tensore dei gate:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]], device='cuda:0')\n",
      "Lo stato corrente: tensor([[0.0645]], device='cuda:0')\n",
      "L'azione eseguita: tensor([[20]], device='cuda:0')\n",
      "Ottimizzazione del modello\n",
      "non_final_next_states shape: torch.Size([8, 1])\n",
      "non_final_tens_gate_used shape: torch.Size([8, 11])\n",
      "non_final_tens_qubit_used shape: torch.Size([8, 4])\n",
      "next_state_full shape: torch.Size([8, 16])\n",
      "state_batch shape: torch.Size([10, 1])\n",
      "tens_gate_used_batch shape: torch.Size([10, 11])\n",
      "tens_qubit_used_batch shape: torch.Size([10, 4])\n",
      "state_full_batch shape: torch.Size([10, 16])\n",
      "state_action_values shape: torch.Size([10, 1])\n",
      "expected_state_action_values shape: torch.Size([10])\n",
      "Perdita: 0.16343951225280762\n",
      "Modello ottimizzato con successo!\n",
      "Pesi aggiornati\n",
      "\n",
      "Forma dello stato: torch.Size([1, 1])\n",
      "Forma dello stato concatenato: torch.Size([1, 16])\n",
      "L'azione selezionata: tensor([[21]], device='cuda:0')\n",
      "Current Quantum Circuit:\n",
      "     ┌─────────────┐     \n",
      "q_0: ┤ Ry(0.82862) ├─────\n",
      "     └─────────────┘     \n",
      "q_1: ────────────────────\n",
      "                    ┌───┐\n",
      "q_2: ───────────────┤ X ├\n",
      "          ┌───┐     └─┬─┘\n",
      "q_3: ─────┤ H ├───────■──\n",
      "          └───┘          \n",
      "[0 1 0 0 0 0 2 1 1 2 1 1 2 1 0 2 1 0 2 2 1 2 2 2 1 2 0 1 0 1 0 1 2 1 1 2 1\n",
      " 1 1 0 2 0 0 0 0 1 1 0 2 0 1 1 2 0]\n",
      "Quantum Circuit:\n",
      "     ┌─────────────┐     \n",
      "q_0: ┤ Ry(0.82862) ├─────\n",
      "     └┬────────────┤     \n",
      "q_1: ─┤ Ry(5.4139) ├─────\n",
      "      └────────────┘┌───┐\n",
      "q_2: ───────────────┤ X ├\n",
      "          ┌───┐     └─┬─┘\n",
      "q_3: ─────┤ H ├───────■──\n",
      "          └───┘          \n",
      "FEATURE MAP:\n",
      "        ┌──────────────────┐┌────────────────────────────────────┐\n",
      "q310_0: ┤0                 ├┤0                                   ├\n",
      "        │                  ││                                    │\n",
      "q310_1: ┤1                 ├┤1                                   ├\n",
      "        │  circuit-4982669 ││  ZZFeatureMap(x[0],x[1],x[2],x[3]) │\n",
      "q310_2: ┤2                 ├┤2                                   ├\n",
      "        │                  ││                                    │\n",
      "q310_3: ┤3                 ├┤3                                   ├\n",
      "        └──────────────────┘└────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "\"\"\"import torch\n",
    "from itertools import count\n",
    "\n",
    "if torch.cuda.is_available() or torch.backends.mps.is_available():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "num_episodes = 50\n",
    "episode_results = []\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    print(\"Episodio numero: \" + str(i_episode + 1) + \"/\" + str(num_episodes))\n",
    "    state, info = env.reset()\n",
    "    print(\"lo stato prima del tensore è: \" + str(state))\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    \n",
    "    for t in count():\n",
    "        print(state.shape)\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, _ = env.step(action)\n",
    "        print(\"La reward sul test set è: \" + str(reward))\n",
    "        print(\"L'episodio risulta essere: \" + str(terminated))\n",
    "        \n",
    "        reward = torch.tensor([reward], dtype=torch.float32, device=device)\n",
    "        done = terminated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "        print(\"Lo stato: \" + str(state))\n",
    "        print(\"L'action è: \" + str(action))\n",
    "        \n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 − τ)θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key] * TAU + target_net_state_dict[key] * (1 - TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "        print(\"Pesi aggiornati\")\n",
    "        \n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            break\n",
    "        \n",
    "        # Store the accuracy and rewards for this episode\n",
    "        episode_results.append({\n",
    "            \"episode\": i_episode,\n",
    "            \"final_reward\": reward.item(),  # Convert to a regular float\n",
    "            \"duration\": t + 1,\n",
    "            \"accuracy\": info.get('current_accuracy')\n",
    "        })\n",
    "\n",
    "# Print all results after the loop\n",
    "print(info.get('circuit'))\n",
    "for result in episode_results:\n",
    "    print(f\"Episode {result['episode']}: Reward: {result['final_reward']}, Duration: {result['duration']}, Accuracy: {result['accuracy']}\")\n",
    "\n",
    "print('Complete')\n",
    "\"\"\"\n",
    "from itertools import count\n",
    "\n",
    "# Parametri di addestramento\n",
    "LR = 1e-3\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 10\n",
    "target_update = 10\n",
    "TAU = 0.005\n",
    "# Epsilon decay parameters\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.05\n",
    "epsilon_decay = 500\n",
    "steps_done = 0\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Inizializza l'ambiente, la rete, l'ottimizzatore e la memoria di replay\n",
    "\n",
    "n_actions = env.get_action_space()\n",
    "state_dim = 1  # Supponendo che lo stato sia un numero reale\n",
    "input_dim = state_dim + len(gate_type_map) + num_qubits  # 1 + num_gate_types + num_qubits\n",
    "input_dim=16\n",
    "policy_net = DQN(input_dim, n_actions).to(device)\n",
    "target_net = DQN(input_dim, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "episode_results = []\n",
    "\n",
    "# Inizio del ciclo di addestramento\n",
    "num_episodes = 15\n",
    "# Inizio del ciclo di addestramento\n",
    "for i_episode in range(num_episodes):\n",
    "    print(f\"\\nEpisodio numero: {i_episode+1}/{num_episodes}\")\n",
    "    state, info = env.reset()\n",
    "    \n",
    "    # Preprocessa lo stato iniziale\n",
    "    state_tensor = preprocess_state(state).to(device)  # Shape: [1]\n",
    "    \n",
    "    # Inizializza le liste per gate e qubit\n",
    "    list_gate_name = []\n",
    "    list_qubit_used = []\n",
    "    \n",
    "    # Inizializza i tensori per gate e qubit\n",
    "    tens_gate_used = encode_gate_info(list_gate_name, device)  # Shape: [1, num_gate_types]\n",
    "    tens_qubit_used = encode_qubit_info(list_qubit_used, num_qubits, device)  # Shape: [1, num_qubits]\n",
    "    \n",
    "    # Stampa diagnostica\n",
    "    print(f\"Forma di tens_gate_used: {tens_gate_used.shape}\")  # [1, num_gate_types]\n",
    "    print(f\"Forma di tens_qubit_used: {tens_qubit_used.shape}\")  # [1, num_qubits]\n",
    "    \n",
    "    done = False\n",
    "    t = 0\n",
    "    while not done:\n",
    "        print(f\"\\nForma dello stato: {state_tensor.shape}\")\n",
    "        \n",
    "        # Prepara lo stato completo concatenato\n",
    "        if state_tensor.dim() == 1:\n",
    "            state_tensor = state_tensor.unsqueeze(0)  # Shape: [1, 1]\n",
    "        \n",
    "        state_full = torch.cat((state_tensor, tens_gate_used, tens_qubit_used), dim=1)  # Shape: [1, input_dim]\n",
    "        print(f\"Forma dello stato concatenato: {state_full.shape}\")\n",
    "        \n",
    "        # Calcola epsilon per la politica epsilon-greedy\n",
    "        eps_threshold = epsilon_end + (epsilon_start - epsilon_end) * math.exp(-1. * steps_done / epsilon_decay)\n",
    "        steps_done += 1\n",
    "        \n",
    "        # Seleziona un'azione basata sullo stato completo\n",
    "        action = select_action(state_full, policy_net, n_actions, device, eps_threshold)\n",
    "        print(f\"L'azione selezionata: {action}\")\n",
    "        \n",
    "        # Esegui l'azione nell'ambiente\n",
    "        observation, reward, terminated, measures = env.step(action.item())\n",
    "        \n",
    "        print(f\"La reward sul test set è: {reward}\")\n",
    "        print(f\"L'episodio risulta essere: {terminated}\")\n",
    "        \n",
    "        # Converti la reward in un tensore float\n",
    "        reward_tensor = torch.tensor([reward], dtype=torch.float32, device=device)\n",
    "        \n",
    "        # Recupera i nomi dei gate e i qubit usati\n",
    "        gate_names_applied = measures.get('gates_applied', [])  # Lista dei nomi dei gate applicati in questo step\n",
    "        qubits_used = measures.get('gate_qubits', [])         # Lista degli indici dei qubit usati in questo step\n",
    "        \n",
    "        # Codifica le informazioni del gate aggiunto in questo step\n",
    "        gate_names_applied = measures.get('gates_applied', [])  # Lista dei nomi dei gate applicati in questo step\n",
    "        qubits_used = measures.get('gate_qubits', [])         # Lista degli indici dei qubit usati in questo step\n",
    "\n",
    "        # Assicurati che gate_names_applied e qubits_used siano liste\n",
    "        if not isinstance(gate_names_applied, list):\n",
    "            gate_names_applied = [gate_names_applied]\n",
    "\n",
    "        if not isinstance(qubits_used, list):\n",
    "            qubits_used = [qubits_used]\n",
    "\n",
    "        # Codifica le informazioni aggiornate per questo step\n",
    "        tens_gate_used = encode_gate_info(gate_names_applied, device)         # Shape: [1, num_gate_types]\n",
    "        tens_qubit_used = encode_qubit_info(qubits_used, num_qubits, device)  # Shape: [1, num_qubits]\n",
    "\n",
    "        # Stampa diagnostica\n",
    "        print(\"Il tensore dei qubit:\")\n",
    "        print(tens_qubit_used)\n",
    "        print(\"Il tensore dei gate:\")\n",
    "        print(tens_gate_used)\n",
    "\n",
    "        \n",
    "        \n",
    "        # Determina se l'episodio è terminato\n",
    "        done = terminated\n",
    "        \n",
    "        # Prepara il prossimo stato\n",
    "        if done:\n",
    "            next_state_tensor = None\n",
    "        else:\n",
    "            next_state = observation  # Supponiamo che 'observation' sia la prossima metrica dello stato\n",
    "            next_state_tensor = preprocess_state(next_state).to(device)  # Shape: [1]\n",
    "        \n",
    "        print(f\"Lo stato corrente: {state_tensor}\")\n",
    "        print(f\"L'azione eseguita: {action}\")\n",
    "        \n",
    "        # Memorizza la transizione nella memoria di replay\n",
    "        memory.push(state_tensor, action, reward_tensor, next_state_tensor, done, tens_gate_used, tens_qubit_used)\n",
    "        \n",
    "        # Aggiorna lo stato corrente\n",
    "        state_tensor = next_state_tensor\n",
    "        \n",
    "        # Ottimizza il modello\n",
    "        optimize_model(policy_net, target_net, memory, optimizer, BATCH_SIZE, GAMMA, device)\n",
    "        \n",
    "        # Soft update della rete target\n",
    "        with torch.no_grad():\n",
    "            for target_param, policy_param in zip(target_net.parameters(), policy_net.parameters()):\n",
    "                target_param.data.copy_(TAU * policy_param.data + (1.0 - TAU) * target_param.data)\n",
    "        print(\"Pesi aggiornati\")\n",
    "        \n",
    "        # Se l'episodio è terminato, registra i risultati\n",
    "        if done:\n",
    "            episode_results.append({\n",
    "                \"episode\": i_episode + 1,\n",
    "                \"final_reward\": reward,\n",
    "                \"duration\": t + 1,\n",
    "                \"accuracy\": measures.get('current_accuracy', 0.0)\n",
    "            })\n",
    "            break\n",
    "        \n",
    "        t += 1\n",
    "    \n",
    "    # Aggiorna la rete target periodicamente (Hard update)\n",
    "    if (i_episode + 1) % target_update == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        print(\"Rete target aggiornata\")\n",
    "\n",
    "# Stampa tutti i risultati dopo il ciclo\n",
    "print(\"\\nCircuito finale:\")\n",
    "print(info.get('circuit'))\n",
    "for result in episode_results:\n",
    "    print(f\"Episodio {result['episode']}: Reward: {result['final_reward']}, Durata: {result['duration']}, Accuracy: {result['accuracy']}\")\n",
    "\n",
    "print('Addestramento Completo')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva il modello addestrato\n",
    "torch.save({\n",
    "    'policy_net_state_dict': policy_net.state_dict(),\n",
    "    'target_net_state_dict': target_net.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, 'rl-wine.pth')\n",
    "print(\"Modello salvato con successo!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ricarica il modello salvato\n",
    "checkpoint = torch.load('rl-wine.pth')\n",
    "policy_net.load_state_dict(checkpoint['policy_net_state_dict'])\n",
    "target_net.load_state_dict(checkpoint['target_net_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "print(\"Modello ricaricato con successo!\")\n",
    "print(policy_net)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_quantum_circuit(agent, env):\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = select_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "# Genera il circuito quantistico\n",
    "test_goal_state = [0j] * (2**num_features - 1) + [1+0j]\n",
    "env.var_init(num_qubits=num_features,\n",
    "             unitary=False,\n",
    "             gate_group='pauli',\n",
    "             connectivity='fully_connected',\n",
    "             X_train=X_train,\n",
    "             Y_train=y_train,\n",
    "             X_test=X_test,\n",
    "             Y_test=y_test,\n",
    "             feature_map=feature_map,\n",
    "             goal_state=test_goal_state)\n",
    "\n",
    "quantum_circuit = generate_quantum_circuit(policy_net, env)\n",
    "print(\"Circuito quantistico generato con successo!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnr_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
