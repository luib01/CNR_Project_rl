{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105, 4) (105,) (45, 4) (45,)\n",
      "[1 1 0 2 1 2 0 0 0 2 2 0 0 1 1 2 0 0 2 1 0 2 2 2 1 0 0 0 1 1 0 0 1 1 0 0 1\n",
      " 2 2 0 2 0 2 0 2 1 0 2 1 2 1 0 1 2 1 2 0 1 0 1 1 1 2 1 1 2 2 0 2 1 1 2 0 2\n",
      " 2 1 0 2 2 0 0 2 2 2 0 2 1 2 2 0 1 1 1 1 1 0 2 1 2 0 0 1 0 1 0]\n",
      "[2 1 2 1 2 2 1 1 0 2 0 0 2 2 0 2 1 0 0 0 1 0 1 2 2 1 1 1 1 0 2 2 1 0 2 0 0\n",
      " 0 0 1 1 0 2 2 1]\n"
     ]
    }
   ],
   "source": [
    "from qiskit import QuantumCircuit\n",
    "from qiskit.visualization import plot_histogram\n",
    "from qiskit_machine_learning.algorithms import VQC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from qiskit.circuit.library import PauliFeatureMap, ZFeatureMap, ZZFeatureMap\n",
    "from qiskit_machine_learning.circuit.library import RawFeatureVector\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# Carica il dataset Iris\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Caricamento dei dati\n",
    "data = load_iris()\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "\n",
    "# Preprocessing\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "print(y_train)\n",
    "print(y_test)\n",
    "\n",
    "# Creazione di un circuito iniziale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature map dimension: 4\n",
      "     ┌────────────────────────────────────┐\n",
      "q_0: ┤0                                   ├\n",
      "     │                                    │\n",
      "q_1: ┤1                                   ├\n",
      "     │  ZZFeatureMap(x[0],x[1],x[2],x[3]) │\n",
      "q_2: ┤2                                   ├\n",
      "     │                                    │\n",
      "q_3: ┤3                                   ├\n",
      "     └────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "num_features = X.shape[1]\n",
    "\n",
    "feature_map = ZZFeatureMap(feature_dimension=num_features, reps=2, entanglement=\"linear\")\n",
    "feature_map.draw(output='mpl')\n",
    "feature_map.decompose().draw(output='mpl')\n",
    "print(\"Feature map dimension:\", feature_map.num_parameters)\n",
    "print(feature_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, num_actions)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Creazione del modello\n",
    "input_shape = 4  # Numero di features dell'input (es. dimensioni dell'osservazione)\n",
    "num_actions = 2  # Numero di azioni possibili\n",
    "\n",
    "model = QNetwork(input_shape, num_actions)\n",
    "\n",
    "# Ottimizzatore e perdita\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, model, optimizer, loss_fn, num_actions, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, gamma=0.99):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.num_actions = num_actions\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.gamma = gamma\n",
    "        self.memory = []\n",
    "        self.batch_size = 32\n",
    "        self.max_memory_size = 1000\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        if len(self.memory) > self.max_memory_size:\n",
    "            self.memory.pop(0)\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        q_values = self.model(state)\n",
    "        return torch.argmax(q_values).item()\n",
    "    \n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        batch = np.random.choice(len(self.memory), self.batch_size, replace=False)\n",
    "        for i in batch:\n",
    "            state, action, reward, next_state, done = self.memory[i]\n",
    "            \n",
    "            state = torch.FloatTensor(state).unsqueeze(0)\n",
    "            next_state = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "            target = reward\n",
    "            \n",
    "            if not done:\n",
    "                target = reward + self.gamma * torch.max(self.model(next_state)).item()\n",
    "            \n",
    "            target_f = self.model(state)\n",
    "            target_f[0][action] = target\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(state)\n",
    "            loss = self.loss_fn(output, target_f)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.90068117 -1.28296331 -0.4308277  -0.13075464]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di qubit per la porta id : 1\n",
      "Numero di qubit per la porta h : 1\n",
      "Numero di qubit per la porta x : 1\n",
      "Numero di qubit per la porta z : 1\n",
      "Numero di qubit per la porta cx : 2\n",
      "Episode 1/100\n",
      "((0,), <bound method QuantumCircuit.z of <qiskit.circuit.quantumcircuit.QuantumCircuit object at 0x146d7d5a0>>)\n",
      "Numero di qubit per la porta id : 1\n",
      "Numero di qubit per la porta h : 1\n",
      "Numero di qubit per la porta x : 1\n",
      "Numero di qubit per la porta z : 1\n",
      "Numero di qubit per la porta cx : 2\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'QuantumCircuit' object has no attribute 'result'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m env\u001b[38;5;241m.\u001b[39mgate_list[action]\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(env\u001b[38;5;241m.\u001b[39mgate_list[action])\n\u001b[0;32m---> 33\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m agent\u001b[38;5;241m.\u001b[39mremember(state, action, reward, next_state, done)\n\u001b[1;32m     36\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cnr_project/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py:13\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 13\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m observation, reward, done, info\n",
      "File \u001b[0;32m~/Desktop/CNR_Project/quantumcircuit_gym-master/CNR_Project_rl/gym_quantcircuit/envs/QuantCircuitEnv.py:284\u001b[0m, in \u001b[0;36mQuantCircuitEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_unitary:\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob \u001b[38;5;241m=\u001b[39m transpile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqcircuit, backend\u001b[38;5;241m=\u001b[39mUnitarySimulator())\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_unitary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m()\u001b[38;5;241m.\u001b[39mget_unitary(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqcircuit)\n\u001b[1;32m    285\u001b[0m     diff \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgoal_unitary \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_unitary)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    286\u001b[0m     diff \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mreal(diff), np\u001b[38;5;241m.\u001b[39mimag(diff))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'QuantumCircuit' object has no attribute 'result'"
     ]
    }
   ],
   "source": [
    "\n",
    "import gym\n",
    "import gym_quantcircuit\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "\n",
    "env = gym.make('quantcircuit-v0')\n",
    "agent = DQNAgent(model, optimizer, loss_fn, num_actions)\n",
    "\n",
    "episodes = 100\n",
    "num_qubits = 4\n",
    "test_goal_state = [0j] * (2**num_qubits - 1) + [1+0j]\n",
    "env.var_init(num_qubits,\n",
    "             unitary=True,\n",
    "             gate_group='pauli',\n",
    "             connectivity='fully_connected',\n",
    "             X_train=X_train,\n",
    "             Y_train=y_train,\n",
    "             X_test=X_test,\n",
    "             Y_test=y_test,\n",
    "             feature_map=feature_map,\n",
    "             goal_state=test_goal_state)\n",
    "\n",
    "for e in range(episodes):\n",
    "    print(f\"Episode {e+1}/{episodes}\")\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = env.sample()\n",
    "        env.gate_list[action]\n",
    "        print(env.gate_list[action])\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            print(f\"Episode {e+1}/{episodes} - Reward: {total_reward}\")\n",
    "            break\n",
    "    \n",
    "    agent.replay()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
